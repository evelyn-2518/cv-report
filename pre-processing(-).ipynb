{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy pandas tensorflow\n",
    "!pip install numpy==1.23.5 pandas==1.5.3 tensorflow==2.10.0 matplotlib pillow tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgZqOWRgaUN8",
    "outputId": "2ec2c5d0-ec94-413f-f7fb-0256d2ddf316",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# åŒ¯å…¥å¥—ä»¶\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "# Global parameters\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LUaVFkLbuZF",
    "outputId": "6efdc084-b8fa-4ce3-9e5f-85705d06f8b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# é¡¯ç¤ºè©³ç´° GPU è¨Šæ¯\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install kaggle --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# å•Ÿç”¨ mixed precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"âœ… Mixed Precision enabled: \", mixed_precision.global_policy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# å»ºç«‹è³‡æ–™å¤¾\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "\n",
    "# å‡è¨­ kaggle.json å·²ç¶“ä¸Šå‚³åˆ°ç•¶å‰ç›®éŒ„\n",
    "!mv kaggle.json /root/.kaggle/\n",
    "\n",
    "# è¨­å®šæ¬Šé™\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "print(\"âœ… kaggle API å·²è¨­å®šå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kaggle datasets list -s deepfashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d hserdaraltan/deepfashion-inshop-clothes-retrieval-adjusted -p /workspace/data\n",
    "!unzip -q /workspace/data/deepfashion-inshop-clothes-retrieval-adjusted.zip -d /workspace/data/deepfashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Vmr46W98dJZU",
    "outputId": "05efb3fd-8cf1-4a91-ed35-1e4248a72d3d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# è³‡æ–™è¼‰å…¥èˆ‡å®Œæ•´æ€§é©—è­‰ \n",
    "\n",
    "print(\"ğŸ” è¼‰å…¥ä¸¦é©—è­‰å½±åƒå’Œé®ç½©...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define the paths for images and masks using the path from the previous cell output\n",
    "# Assuming the images and masks are located in 'images' and 'masks' subdirectories within the downloaded path\n",
    "path = '/workspace/data/deepfashion/'\n",
    "IMAGES_PATH = os.path.join(path, 'images/')\n",
    "MASKS_PATH = os.path.join(path, 'masks/')\n",
    "\n",
    "# å‡è¨­ä½ å·²ç¶“å°‡è³‡æ–™é›†è§£å£“ç¸®åˆ° /workspace/data/deepfashion\n",
    "path = '/workspace/data/deepfashion/'\n",
    "\n",
    "def load_and_validate_images(path, file_type=\"images\"):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥ä¸¦é©—è­‰å½±åƒæª”æ¡ˆ\n",
    "\n",
    "    æª¢æŸ¥é …ç›®ï¼š\n",
    "    1. æª”æ¡ˆæ˜¯å¦å¯è®€å–\n",
    "    2. å°ºå¯¸æ˜¯å¦åˆç†ï¼ˆ> 50x50ï¼‰\n",
    "    3. æ˜¯å¦ç‚ºç©ºç™½å½±åƒï¼ˆæ¨™æº–å·® < 1ï¼‰\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“‚ è¼‰å…¥ {file_type}...\")\n",
    "\n",
    "    # å–å¾—æ‰€æœ‰æª”æ¡ˆ\n",
    "    files = glob.glob(path + '*')\n",
    "    files.sort()\n",
    "\n",
    "    valid_files = []\n",
    "    corrupted_files = []\n",
    "    stats = {\n",
    "        'total': len(files),\n",
    "        'valid': 0,\n",
    "        'corrupted': 0,\n",
    "        'too_small': 0,\n",
    "        'blank': 0,\n",
    "        'read_error': 0\n",
    "    }\n",
    "\n",
    "    # é©—è­‰æ¯å€‹æª”æ¡ˆ\n",
    "    for file_path in tqdm(files, desc=f\"Validating {file_type}\"):\n",
    "        try:\n",
    "            # é–‹å•Ÿä¸¦é©—è­‰å½±åƒ\n",
    "            img = Image.open(file_path)\n",
    "            img.verify()  # é©—è­‰æª”æ¡ˆå®Œæ•´æ€§\n",
    "\n",
    "            # é‡æ–°é–‹å•Ÿä»¥è®€å–è³‡æ–™ï¼ˆverifyå¾Œéœ€è¦é‡æ–°é–‹å•Ÿï¼‰\n",
    "            img = Image.open(file_path)\n",
    "            width, height = img.size\n",
    "\n",
    "            # æª¢æŸ¥å°ºå¯¸\n",
    "            if width < 50 or height < 50:\n",
    "                corrupted_files.append((file_path, \"too_small\"))\n",
    "                stats['too_small'] += 1\n",
    "                continue\n",
    "\n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºç™½å½±åƒ\n",
    "            img_array = np.array(img)\n",
    "            if img_array.std() < 1:\n",
    "                corrupted_files.append((file_path, \"blank\"))\n",
    "                stats['blank'] += 1\n",
    "                continue\n",
    "\n",
    "            # é€šéæ‰€æœ‰æª¢æŸ¥\n",
    "            valid_files.append(file_path)\n",
    "            stats['valid'] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            corrupted_files.append((file_path, f\"error: {str(e)[:30]}\"))\n",
    "            stats['read_error'] += 1\n",
    "\n",
    "    stats['corrupted'] = stats['too_small'] + stats['blank'] + stats['read_error']\n",
    "\n",
    "    return valid_files, corrupted_files, stats\n",
    "\n",
    "# é©—è­‰å½±åƒ\n",
    "valid_images, corrupted_images, image_stats = load_and_validate_images(\n",
    "    IMAGES_PATH, \"images\"\n",
    ")\n",
    "\n",
    "# é©—è­‰é®ç½©\n",
    "valid_masks, corrupted_masks, mask_stats = load_and_validate_images(\n",
    "    MASKS_PATH, \"masks\"\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºçµ±è¨ˆçµæœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š é©—è­‰çµæœæ‘˜è¦:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ–¼ï¸ å½±åƒæª”æ¡ˆ:\")\n",
    "print(f\"  ç¸½æ•¸é‡: {image_stats['total']}\")\n",
    "# Add a check to prevent division by zero\n",
    "if image_stats['total'] > 0:\n",
    "    print(f\"  âœ… æœ‰æ•ˆ: {image_stats['valid']} ({image_stats['valid']/image_stats['total']*100:.1f}%)\")\n",
    "    print(f\"  âŒ æå£: {image_stats['corrupted']} ({image_stats['corrupted']/image_stats['total']*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"  âœ… æœ‰æ•ˆ: 0 (0.0%)\")\n",
    "    print(\"  âŒ æå£: 0 (0.0%)\")\n",
    "\n",
    "if image_stats['corrupted'] > 0:\n",
    "    print(f\"     - å°ºå¯¸éå°: {image_stats['too_small']}\")\n",
    "    print(f\"     - ç©ºç™½å½±åƒ: {image_stats['blank']}\")\n",
    "    print(f\"     - è®€å–éŒ¯èª¤: {image_stats['read_error']}\")\n",
    "\n",
    "print(\"\\nğŸ­ é®ç½©æª”æ¡ˆ:\")\n",
    "print(f\"  ç¸½æ•¸é‡: {mask_stats['total']}\")\n",
    "# Add a check to prevent division by zero\n",
    "if mask_stats['total'] > 0:\n",
    "    print(f\"  âœ… æœ‰æ•ˆ: {mask_stats['valid']} ({mask_stats['valid']/mask_stats['total']*100:.1f}%)\")\n",
    "    print(f\"  âŒ æå£: {mask_stats['corrupted']} ({mask_stats['corrupted']/mask_stats['total']*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"  âœ… æœ‰æ•ˆ: 0 (0.0%)\")\n",
    "    print(\"  âŒ æå£: 0 (0.0%)\")\n",
    "\n",
    "if mask_stats['corrupted'] > 0:\n",
    "    print(f\"     - å°ºå¯¸éå°: {mask_stats['too_small']}\")\n",
    "    print(f\"     - ç©ºç™½å½±åƒ: {mask_stats['blank']}\")\n",
    "    print(f\"     - è®€å–éŒ¯èª¤: {mask_stats['read_error']}\")\n",
    "\n",
    "# é¡¯ç¤ºæå£æª”æ¡ˆç¯„ä¾‹\n",
    "if len(corrupted_images) > 0:\n",
    "    print(\"\\nâš ï¸ æå£çš„å½±åƒæª”æ¡ˆç¯„ä¾‹ï¼ˆå‰5å€‹ï¼‰:\")\n",
    "    for i, (file_path, reason) in enumerate(corrupted_images[:5]):\n",
    "        print(f\"  {i+1}. {os.path.basename(file_path)} - {reason}\")\n",
    "\n",
    "if len(corrupted_masks) > 0:\n",
    "    print(\"\\nâš ï¸ æå£çš„é®ç½©æª”æ¡ˆç¯„ä¾‹ï¼ˆå‰5å€‹ï¼‰:\")\n",
    "    for i, (file_path, reason) in enumerate(corrupted_masks[:5]):\n",
    "        print(f\"  {i+1}. {os.path.basename(file_path)} - {reason}\")\n",
    "\n",
    "# è¦–è¦ºåŒ–é©—è­‰çµæœ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# å½±åƒçµ±è¨ˆ\n",
    "labels_img = ['æœ‰æ•ˆ', 'å°ºå¯¸éå°', 'ç©ºç™½', 'è®€å–éŒ¯èª¤']\n",
    "sizes_img = [image_stats['valid'], image_stats['too_small'],\n",
    "             image_stats['blank'], image_stats['read_error']]\n",
    "colors_img = ['#4CAF50', '#FFC107', '#FF9800', '#F44336']\n",
    "# Add a check to prevent plotting when there are no images\n",
    "if image_stats['total'] > 0:\n",
    "    axes[0].pie(sizes_img, labels=labels_img, colors=colors_img, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('å½±åƒæª”æ¡ˆç‹€æ…‹åˆ†ä½ˆ')\n",
    "\n",
    "# é®ç½©çµ±è¨ˆ\n",
    "sizes_mask = [mask_stats['valid'], mask_stats['too_small'],\n",
    "              mask_stats['blank'], mask_stats['read_error']]\n",
    "colors_mask = ['#4CAF50', '#FFC107', '#FF9800', '#F44336'] # Define colors_mask\n",
    "# Add a check to prevent plotting when there are no masks\n",
    "if mask_stats['total'] > 0:\n",
    "    axes[1].pie(sizes_mask, labels=labels_img, colors=colors_mask, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('é®ç½©æª”æ¡ˆç‹€æ…‹åˆ†ä½ˆ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"âœ… é©—è­‰å®Œæˆï¼å…±æ‰¾åˆ° {len(valid_images)} å€‹æœ‰æ•ˆå½±åƒå’Œ {len(valid_masks)} å€‹æœ‰æ•ˆé®ç½©\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "329a2dcd",
    "outputId": "b6dfb91e-c02a-4e03-8073-39a2489d5d31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get update -qq\n",
    "!apt-get install fonts-noto-cjk -qq\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Configure matplotlib to use the Noto Sans CJK font\n",
    "for font in fm.findSystemFonts(fontpaths=['/usr/share/fonts/opentype/noto/']):\n",
    "    if 'NotoSansCJK' in font:\n",
    "        fm.fontManager.addfont(font)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Noto Sans CJK JP'\n",
    "plt.rcParams['axes.unicode_minus'] = False # Allow negative signs to be displayed\n",
    "\n",
    "print(\"âœ… Noto Sans CJK font installed and configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7on8aOpBi6pn",
    "outputId": "c0321295-2fee-4fa7-ec1c-35687ed65275",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# å½±åƒ-é®ç½©é…å°é©—è­‰\n",
    "print(\"ğŸ”— é©—è­‰å½±åƒå’Œé®ç½©çš„é…å°é—œä¿‚...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def verify_image_mask_pairs(image_paths, mask_paths):\n",
    "    \"\"\"\n",
    "    é©—è­‰å½±åƒå’Œé®ç½©æ˜¯å¦æ­£ç¢ºé…å°\n",
    "\n",
    "    æª¢æŸ¥ï¼š\n",
    "    1. æª”åæ˜¯å¦ä¸€è‡´ï¼ˆå»é™¤å‰¯æª”åï¼‰\n",
    "    2. æ•¸é‡æ˜¯å¦ç›¸åŒ\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ” æª¢æŸ¥é…å°é—œä¿‚...\")\n",
    "\n",
    "    # å»ºç«‹æª”ååˆ°è·¯å¾‘çš„æ˜ å°„\n",
    "    image_dict = {}\n",
    "    for img_path in image_paths:\n",
    "        filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        image_dict[filename] = img_path\n",
    "\n",
    "    mask_dict = {}\n",
    "    for mask_path in mask_paths:\n",
    "        filename = os.path.splitext(os.path.basename(mask_path))[0]\n",
    "        mask_dict[filename] = mask_path\n",
    "\n",
    "    # æ‰¾å‡ºé…å°å’Œæœªé…å°çš„æª”æ¡ˆ\n",
    "    image_names = set(image_dict.keys())\n",
    "    mask_names = set(mask_dict.keys())\n",
    "\n",
    "    # å®Œç¾é…å°çš„æª”æ¡ˆ\n",
    "    paired_names = image_names & mask_names\n",
    "\n",
    "    # åªæœ‰å½±åƒæ²’æœ‰é®ç½©\n",
    "    only_images = image_names - mask_names\n",
    "\n",
    "    # åªæœ‰é®ç½©æ²’æœ‰å½±åƒ\n",
    "    only_masks = mask_names - image_names\n",
    "\n",
    "    # å»ºç«‹é…å°åˆ—è¡¨\n",
    "    paired_images = []\n",
    "    paired_masks = []\n",
    "    for name in sorted(paired_names):\n",
    "        paired_images.append(image_dict[name])\n",
    "        paired_masks.append(mask_dict[name])\n",
    "\n",
    "    return paired_images, paired_masks, only_images, only_masks\n",
    "\n",
    "# åŸ·è¡Œé…å°é©—è­‰\n",
    "paired_images, paired_masks, only_images, only_masks = verify_image_mask_pairs(\n",
    "    valid_images, valid_masks\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºé…å°çµæœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š é…å°é©—è­‰çµæœ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nâœ… æˆåŠŸé…å°: {len(paired_images)} å°\")\n",
    "print(f\"âš ï¸ åªæœ‰å½±åƒï¼ˆç¼ºå°‘é®ç½©ï¼‰: {len(only_images)} å€‹\")\n",
    "print(f\"âš ï¸ åªæœ‰é®ç½©ï¼ˆç¼ºå°‘å½±åƒï¼‰: {len(only_masks)} å€‹\")\n",
    "\n",
    "# é¡¯ç¤ºé…å°ç¯„ä¾‹\n",
    "print(\"\\nğŸ“‹ é…å°ç¯„ä¾‹ï¼ˆå‰5å°ï¼‰:\")\n",
    "for i in range(min(5, len(paired_images))):\n",
    "    img_name = os.path.basename(paired_images[i])\n",
    "    mask_name = os.path.basename(paired_masks[i])\n",
    "    print(f\"  {i+1}. {img_name} â†” {mask_name}\")\n",
    "\n",
    "# é¡¯ç¤ºæœªé…å°æª”æ¡ˆ\n",
    "if len(only_images) > 0:\n",
    "    print(\"\\nâš ï¸ ç¼ºå°‘é®ç½©çš„å½±åƒï¼ˆå‰5å€‹ï¼‰:\")\n",
    "    for i, name in enumerate(list(only_images)[:5]):\n",
    "        print(f\"  {i+1}. {name}\")\n",
    "\n",
    "if len(only_masks) > 0:\n",
    "    print(\"\\nâš ï¸ ç¼ºå°‘å½±åƒçš„é®ç½©ï¼ˆå‰5å€‹ï¼‰:\")\n",
    "    for i, name in enumerate(list(only_masks)[:5]):\n",
    "        print(f\"  {i+1}. {name}\")\n",
    "\n",
    "# è¦–è¦ºåŒ–é…å°ç‹€æ…‹\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = ['å®Œç¾é…å°', 'ç¼ºå°‘é®ç½©', 'ç¼ºå°‘å½±åƒ']\n",
    "counts = [len(paired_images), len(only_images), len(only_masks)]\n",
    "colors = ['#4CAF50', '#FF9800', '#F44336']\n",
    "\n",
    "bars = ax.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# åœ¨æŸ±ç‹€åœ–ä¸Šé¡¯ç¤ºæ•¸å€¼\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('æ•¸é‡', fontsize=12)\n",
    "ax.set_title('å½±åƒ-é®ç½©é…å°ç‹€æ…‹', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# é¡¯ç¤ºé…å°ç‡\n",
    "pairing_rate = len(paired_images) / len(valid_images) * 100 if len(valid_images) > 0 else 0\n",
    "print(f\"\\nğŸ“ˆ é…å°æˆåŠŸç‡: {pairing_rate:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"âœ… é…å°é©—è­‰å®Œæˆï¼å°‡ä½¿ç”¨ {len(paired_images)} å°è³‡æ–™é€²è¡Œè¨“ç·´\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å„²å­˜é…å°è³‡è¨Šä¾›å¾ŒçºŒä½¿ç”¨\n",
    "print(\"\\nğŸ’¾ å„²å­˜é…å°è³‡è¨Š...\")\n",
    "final_images = paired_images\n",
    "final_masks = paired_masks\n",
    "print(f\"  æœ€çµ‚è³‡æ–™é‡: {len(final_images)} å°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iFskituyjFWx",
    "outputId": "9184ac06-ff8f-4406-9daa-e0e8cace63b4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Cell 5: è¦–è¦ºåŒ–æ¨£æœ¬è³‡æ–™ ====================\n",
    "print(\"ğŸ¨ è¦–è¦ºåŒ–åŸå§‹è³‡æ–™æ¨£æœ¬...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def visualize_samples(image_paths, mask_paths, num_samples=4):\n",
    "    \"\"\"\n",
    "    è¦–è¦ºåŒ–å½±åƒå’Œå°æ‡‰çš„é®ç½©\n",
    "    \"\"\"\n",
    "    # éš¨æ©Ÿé¸æ“‡æ¨£æœ¬\n",
    "    indices = random.sample(range(len(image_paths)), min(num_samples, len(image_paths)))\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 4))\n",
    "\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # è®€å–å½±åƒå’Œé®ç½©\n",
    "        img = Image.open(image_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(mask_paths[idx])\n",
    "\n",
    "        # è½‰æ›ç‚º numpy array\n",
    "        img_array = np.array(img)\n",
    "        mask_array = np.array(mask)\n",
    "\n",
    "        # é¡¯ç¤ºåŸå§‹å½±åƒ\n",
    "        axes[i, 0].imshow(img_array)\n",
    "        axes[i, 0].set_title(f'åŸå§‹å½±åƒ\\n{os.path.basename(image_paths[idx])}',\n",
    "                            fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # é¡¯ç¤ºé®ç½©ï¼ˆå½©è‰²æ˜ å°„ï¼‰\n",
    "        im = axes[i, 1].imshow(mask_array, cmap='jet', vmin=0, vmax=255)\n",
    "        axes[i, 1].set_title(f'é®ç½©ï¼ˆå½©è‰²ï¼‰\\n{os.path.basename(mask_paths[idx])}',\n",
    "                            fontsize=10)\n",
    "        axes[i, 1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
    "\n",
    "        # é¡¯ç¤ºç–ŠåŠ æ•ˆæœ\n",
    "        overlay = img_array.copy()\n",
    "        mask_colored = plt.cm.jet(mask_array / 255.0)[:, :, :3]\n",
    "        overlay = (overlay * 0.6 + mask_colored * 255 * 0.4).astype(np.uint8)\n",
    "        axes[i, 2].imshow(overlay)\n",
    "        axes[i, 2].set_title('ç–ŠåŠ æ•ˆæœ', fontsize=10)\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "        # é¡¯ç¤ºå½±åƒè³‡è¨Š\n",
    "        print(f\"\\næ¨£æœ¬ {i+1}:\")\n",
    "        print(f\"  å½±åƒå°ºå¯¸: {img_array.shape}\")\n",
    "        print(f\"  é®ç½©å°ºå¯¸: {mask_array.shape}\")\n",
    "        print(f\"  é®ç½©å”¯ä¸€å€¼: {np.unique(mask_array)}\")\n",
    "        print(f\"  å½±åƒåƒç´ ç¯„åœ: [{img_array.min()}, {img_array.max()}]\")\n",
    "        print(f\"  é®ç½©åƒç´ ç¯„åœ: [{mask_array.min()}, {mask_array.max()}]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# è¦–è¦ºåŒ–4å€‹éš¨æ©Ÿæ¨£æœ¬\n",
    "print(\"\\nğŸ–¼ï¸ éš¨æ©Ÿé¸æ“‡ 4 å€‹æ¨£æœ¬é€²è¡Œè¦–è¦ºåŒ–...\")\n",
    "visualize_samples(final_images, final_masks, num_samples=4)\n",
    "\n",
    "# çµ±è¨ˆå½±åƒå°ºå¯¸åˆ†ä½ˆ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ çµ±è¨ˆåŸå§‹å½±åƒå°ºå¯¸åˆ†ä½ˆ...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sizes = []\n",
    "for img_path in tqdm(final_images[:1000], desc=\"Scanning image sizes\"):  # æŠ½æ¨£1000å¼µ\n",
    "    img = Image.open(img_path)\n",
    "    sizes.append(img.size)\n",
    "\n",
    "widths = [s[0] for s in sizes]\n",
    "heights = [s[1] for s in sizes]\n",
    "\n",
    "print(f\"\\nå¯¬åº¦çµ±è¨ˆ:\")\n",
    "print(f\"  æœ€å°: {min(widths)}px\")\n",
    "print(f\"  æœ€å¤§: {max(widths)}px\")\n",
    "print(f\"  å¹³å‡: {np.mean(widths):.0f}px\")\n",
    "print(f\"  ä¸­ä½æ•¸: {np.median(widths):.0f}px\")\n",
    "\n",
    "print(f\"\\né«˜åº¦çµ±è¨ˆ:\")\n",
    "print(f\"  æœ€å°: {min(heights)}px\")\n",
    "print(f\"  æœ€å¤§: {max(heights)}px\")\n",
    "print(f\"  å¹³å‡: {np.mean(heights):.0f}px\")\n",
    "print(f\"  ä¸­ä½æ•¸: {np.median(heights):.0f}px\")\n",
    "\n",
    "# è¦–è¦ºåŒ–å°ºå¯¸åˆ†ä½ˆ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(widths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.mean(widths), color='red', linestyle='--',\n",
    "                linewidth=2, label=f'å¹³å‡: {np.mean(widths):.0f}px')\n",
    "axes[0].set_xlabel('å¯¬åº¦ (pixels)', fontsize=12)\n",
    "axes[0].set_ylabel('æ•¸é‡', fontsize=12)\n",
    "axes[0].set_title('å½±åƒå¯¬åº¦åˆ†ä½ˆ', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].hist(heights, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(np.mean(heights), color='red', linestyle='--',\n",
    "                linewidth=2, label=f'å¹³å‡: {np.mean(heights):.0f}px')\n",
    "axes[1].set_xlabel('é«˜åº¦ (pixels)', fontsize=12)\n",
    "axes[1].set_ylabel('æ•¸é‡', fontsize=12)\n",
    "axes[1].set_title('å½±åƒé«˜åº¦åˆ†ä½ˆ', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… æ¨£æœ¬è¦–è¦ºåŒ–å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uZsPUIZVjRiJ",
    "outputId": "a2dea349-04cd-4829-e6f6-b3fecb99e3f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  æ¨™ç±¤åˆ†æèˆ‡é¡åˆ¥çµ±è¨ˆ \n",
    "\n",
    "print(\"ğŸ“Š åˆ†æé®ç½©æ¨™ç±¤èˆ‡é¡åˆ¥åˆ†ä½ˆ...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install and configure fonts-noto-cjk for displaying Chinese characters in plots\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-noto-cjk -qq\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Configure matplotlib to use the Noto Sans CJK font\n",
    "for font in fm.findSystemFonts(fontpaths=['/usr/share/fonts/opentype/noto/']):\n",
    "    if 'NotoSansCJK' in font:\n",
    "        fm.fontManager.addfont(font)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Noto Sans CJK JP'\n",
    "plt.rcParams['axes.unicode_minus'] = False # Allow negative signs to be displayed\n",
    "\n",
    "print(\"âœ… Noto Sans CJK font installed and configured for this cell.\")\n",
    "\n",
    "def analyze_mask_labels(mask_paths, sample_size=None):\n",
    "    \"\"\"\n",
    "    æ·±åº¦åˆ†æé®ç½©æ¨™ç±¤\n",
    "\n",
    "    çµ±è¨ˆï¼š\n",
    "    1. æ‰€æœ‰å”¯ä¸€é¡åˆ¥\n",
    "    2. æ¯å€‹é¡åˆ¥çš„åƒç´ ç¸½æ•¸\n",
    "    3. æ¯å€‹é¡åˆ¥å‡ºç¾åœ¨å¤šå°‘å¼µå½±åƒä¸­\n",
    "    4. é¡åˆ¥ä¸å¹³è¡¡ç¨‹åº¦\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”¬ æƒæé®ç½©æª”æ¡ˆä¸­çš„æ¨™ç±¤...\")\n",
    "\n",
    "    if sample_size:\n",
    "        mask_paths = random.sample(mask_paths, min(sample_size, len(mask_paths)))\n",
    "        print(f\"  ä½¿ç”¨ {len(mask_paths)} å€‹æ¨£æœ¬é€²è¡Œåˆ†æ\")\n",
    "\n",
    "    unique_labels_set = set()\n",
    "    class_pixel_counts = defaultdict(int)\n",
    "    class_occurrence = defaultdict(int)\n",
    "\n",
    "    for mask_path in tqdm(mask_paths, desc=\"Analyzing masks\"):\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        unique_labels = np.unique(mask)\n",
    "\n",
    "        unique_labels_set.update(unique_labels)\n",
    "\n",
    "        # çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„åƒç´ æ•¸\n",
    "        for label in unique_labels:\n",
    "            pixel_count = np.sum(mask == label)\n",
    "            class_pixel_counts[int(label)] += pixel_count\n",
    "            class_occurrence[int(label)] += 1\n",
    "\n",
    "    # è½‰æ›ç‚ºæ’åºåˆ—è¡¨\n",
    "    unique_labels_list = sorted(list(unique_labels_set))\n",
    "    num_classes = len(unique_labels_list)\n",
    "\n",
    "    return num_classes, unique_labels_list, class_pixel_counts, class_occurrence\n",
    "\n",
    "# åŸ·è¡Œæ¨™ç±¤åˆ†æï¼ˆä½¿ç”¨å…¨éƒ¨è³‡æ–™ï¼‰\n",
    "NUM_CLASSES, unique_labels, pixel_counts, occurrences = analyze_mask_labels(\n",
    "    final_masks, sample_size=None\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ˆ æ¨™ç±¤åˆ†æçµæœ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ¯ é¡åˆ¥ç¸½æ•¸: {NUM_CLASSES}\")\n",
    "print(f\"ğŸ·ï¸ æ‰€æœ‰é¡åˆ¥æ¨™ç±¤: {unique_labels}\")\n",
    "\n",
    "# å»ºç«‹è©³ç´°çµ±è¨ˆè¡¨æ ¼\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‹ è©³ç´°é¡åˆ¥çµ±è¨ˆ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stats_data = []\n",
    "total_pixels = sum(pixel_counts.values())\n",
    "\n",
    "for label in unique_labels:\n",
    "    pixels = pixel_counts[label]\n",
    "    occur = occurrences[label]\n",
    "    percentage = (pixels / total_pixels) * 100\n",
    "    avg_pixels = pixels / occur if occur > 0 else 0\n",
    "\n",
    "    stats_data.append({\n",
    "        'é¡åˆ¥ID': label,\n",
    "        'å‡ºç¾æ¬¡æ•¸': occur,\n",
    "        'ç¸½åƒç´ æ•¸': f'{pixels:,}',\n",
    "        'åƒç´ ä½”æ¯”': f'{percentage:.2f}%',\n",
    "        'å¹³å‡åƒç´ ': f'{avg_pixels:,.0f}'\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_data)\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "# æª¢æ¸¬é¡åˆ¥ä¸å¹³è¡¡\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš–ï¸ é¡åˆ¥å¹³è¡¡åˆ†æ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pixel_values = list(pixel_counts.values())\n",
    "max_pixels = max(pixel_values)\n",
    "min_pixels = min(pixel_values)\n",
    "imbalance_ratio = max_pixels / min_pixels if min_pixels > 0 else float('inf')\n",
    "\n",
    "print(f\"æœ€å¤šåƒç´ çš„é¡åˆ¥: {max_pixels:,} åƒç´ \")\n",
    "print(f\"æœ€å°‘åƒç´ çš„é¡åˆ¥: {min_pixels:,} åƒç´ \")\n",
    "print(f\"ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 100:\n",
    "    print(\"\\nâš ï¸ è­¦å‘Š: æª¢æ¸¬åˆ°åš´é‡çš„é¡åˆ¥ä¸å¹³è¡¡ï¼\")\n",
    "    print(\"   å»ºè­°ä½¿ç”¨ä»¥ä¸‹ç­–ç•¥:\")\n",
    "    print(\"   - åŠ æ¬Šæå¤±å‡½æ•¸ (Weighted Loss)\")\n",
    "    print(\"   - éæ¡æ¨£å°‘æ•¸é¡åˆ¥ (Oversampling)\")\n",
    "    print(\"   - Focal Loss\")\n",
    "elif imbalance_ratio > 10:\n",
    "    print(\"\\nâš ï¸ æ³¨æ„: æª¢æ¸¬åˆ°ä¸­åº¦é¡åˆ¥ä¸å¹³è¡¡\")\n",
    "    print(\"   å»ºè­°ä½¿ç”¨åŠ æ¬Šæå¤±å‡½æ•¸\")\n",
    "else:\n",
    "    print(\"\\nâœ… é¡åˆ¥åˆ†ä½ˆç›¸å°å¹³è¡¡\")\n",
    "\n",
    "# è¦–è¦ºåŒ–é¡åˆ¥åˆ†ä½ˆ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. é¡åˆ¥å‡ºç¾æ¬¡æ•¸\n",
    "ax1 = axes[0, 0]\n",
    "labels_list = [str(l) for l in unique_labels]\n",
    "occur_list = [occurrences[l] for l in unique_labels]\n",
    "bars1 = ax1.bar(labels_list, occur_list, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('é¡åˆ¥ ID', fontsize=12)\n",
    "ax1.set_ylabel('å‡ºç¾æ¬¡æ•¸', fontsize=12)\n",
    "ax1.set_title('å„é¡åˆ¥å‡ºç¾é »ç‡', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. é¡åˆ¥åƒç´ ä½”æ¯”\n",
    "ax2 = axes[0, 1]\n",
    "pixel_list = [pixel_counts[l] for l in unique_labels]\n",
    "bars2 = ax2.bar(labels_list, pixel_list, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('é¡åˆ¥ ID', fontsize=12)\n",
    "ax2.set_ylabel('ç¸½åƒç´ æ•¸', fontsize=12)\n",
    "ax2.set_title('å„é¡åˆ¥åƒç´ ç¸½é‡', fontsize=14, fontweight='bold')\n",
    "ax2.set_yscale('log')  # ä½¿ç”¨å°æ•¸åˆ»åº¦ä»¥æ›´å¥½åœ°é¡¯ç¤ºå·®ç•°\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# 3. åœ“é¤…åœ– - åƒç´ ä½”æ¯”ï¼ˆåªé¡¯ç¤ºå‰10å¤§é¡åˆ¥ï¼‰\n",
    "ax3 = axes[1, 0]\n",
    "sorted_labels = sorted(unique_labels, key=lambda x: pixel_counts[x], reverse=True)\n",
    "top_10_labels = sorted_labels[:10]\n",
    "top_10_pixels = [pixel_counts[l] for l in top_10_labels]\n",
    "if len(sorted_labels) > 10:\n",
    "    other_pixels = sum([pixel_counts[l] for l in sorted_labels[10:]])\n",
    "    top_10_labels.append('å…¶ä»–')\n",
    "    top_10_pixels.append(other_pixels)\n",
    "\n",
    "colors = plt.cm.Set3(range(len(top_10_labels)))\n",
    "wedges, texts, autotexts = ax3.pie(top_10_pixels, labels=top_10_labels, autopct='%1.1f%%',\n",
    "                                     colors=colors, startangle=90)\n",
    "ax3.set_title('åƒç´ ä½”æ¯”åˆ†ä½ˆï¼ˆå‰10é¡åˆ¥ï¼‰', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. å¹³å‡åƒç´ æ•¸\n",
    "ax4 = axes[1, 1]\n",
    "avg_pixel_list = [pixel_counts[l] / occurrences[l] if occurrences[l] > 0 else 0\n",
    "                  for l in unique_labels]\n",
    "bars4 = ax4.bar(labels_list, avg_pixel_list, color='mediumseagreen',\n",
    "                edgecolor='black', alpha=0.7)\n",
    "ax4.set_xlabel('é¡åˆ¥ ID', fontsize=12)\n",
    "ax4.set_ylabel('å¹³å‡åƒç´ æ•¸', fontsize=12)\n",
    "ax4.set_title('å„é¡åˆ¥å¹³å‡åƒç´ æ•¸', fontsize=14, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"âœ… æ¨™ç±¤åˆ†æå®Œæˆï¼å…±æ‰¾åˆ° {NUM_CLASSES} å€‹é¡åˆ¥\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å„²å­˜é¡åˆ¥è³‡è¨Š\n",
    "print(\"\\nğŸ’¾ å„²å­˜é¡åˆ¥çµ±è¨ˆè³‡è¨Š...\")\n",
    "class_info = {\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'unique_labels': unique_labels,\n",
    "    'pixel_counts': dict(pixel_counts),\n",
    "    'occurrences': dict(occurrences),\n",
    "    'imbalance_ratio': imbalance_ratio\n",
    "}\n",
    "print(\"  âœ… é¡åˆ¥è³‡è¨Šå·²å„²å­˜åˆ°è®Šæ•¸ 'class_info'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uw9RT1uKm6cM",
    "outputId": "6c53637f-82eb-44f5-f8c6-8cab36c007eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  è¨ˆç®—é¡åˆ¥æ¬Šé‡ \n",
    "\n",
    "print(\"âš–ï¸ è¨ˆç®—é¡åˆ¥æ¬Šé‡ä»¥è™•ç†ä¸å¹³è¡¡å•é¡Œ...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_class_weights(pixel_counts, unique_labels):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—é¡åˆ¥æ¬Šé‡ï¼ˆåé »ç‡åŠ æ¬Šï¼‰\n",
    "\n",
    "    å…¬å¼: weight = total_pixels / (num_classes Ã— class_pixels)\n",
    "    ç„¶å¾Œæ­£è¦åŒ–åˆ° [0.1, 10] ç¯„åœé¿å…æ¥µç«¯å€¼\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”¢ ä½¿ç”¨åé »ç‡åŠ æ¬Šæ–¹æ³•...\")\n",
    "\n",
    "    total_pixels = sum(pixel_counts.values())\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    # è¨ˆç®—åŸå§‹æ¬Šé‡\n",
    "    raw_weights = {}\n",
    "    for label in unique_labels:\n",
    "        class_pixels = pixel_counts[label]\n",
    "        if class_pixels > 0:\n",
    "            raw_weights[label] = total_pixels / (num_classes * class_pixels)\n",
    "        else:\n",
    "            raw_weights[label] = 1.0\n",
    "\n",
    "    # æ­£è¦åŒ–æ¬Šé‡ï¼ˆé¿å…æ¥µç«¯å€¼ï¼‰\n",
    "    max_weight = max(raw_weights.values())\n",
    "    min_weight = min(raw_weights.values())\n",
    "\n",
    "    normalized_weights = {}\n",
    "    for label, weight in raw_weights.items():\n",
    "        # æ­£è¦åŒ–åˆ° [0.1, 10] ç¯„åœ\n",
    "        normalized = 0.1 + (weight - min_weight) / (max_weight - min_weight) * 9.9\n",
    "        normalized_weights[label] = normalized\n",
    "\n",
    "    return normalized_weights, raw_weights\n",
    "\n",
    "# è¨ˆç®—æ¬Šé‡\n",
    "class_weights, raw_weights = compute_class_weights(pixel_counts, unique_labels)\n",
    "\n",
    "# é¡¯ç¤ºæ¬Šé‡\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š é¡åˆ¥æ¬Šé‡çµæœ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "weight_data = []\n",
    "for label in unique_labels:\n",
    "    weight_data.append({\n",
    "        'é¡åˆ¥ID': label,\n",
    "        'åŸå§‹æ¬Šé‡': f'{raw_weights[label]:.4f}',\n",
    "        'æ­£è¦åŒ–æ¬Šé‡': f'{class_weights[label]:.4f}',\n",
    "        'åƒç´ æ•¸': f'{pixel_counts[label]:,}',\n",
    "        'æ¬Šé‡èªªæ˜': 'é«˜' if class_weights[label] > 5 else 'ä¸­' if class_weights[label] > 2 else 'ä½'\n",
    "    })\n",
    "\n",
    "df_weights = pd.DataFrame(weight_data)\n",
    "print(df_weights.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ æ¬Šé‡è§£é‡‹:\")\n",
    "print(\"  - æ¬Šé‡è¶Šé«˜ = è©²é¡åˆ¥è¶Šç¨€æœ‰ = æ¨¡å‹æœƒæ›´é‡è¦–è©²é¡åˆ¥çš„éŒ¯èª¤\")\n",
    "print(\"  - æ¬Šé‡è¶Šä½ = è©²é¡åˆ¥è¶Šå¸¸è¦‹ = é¿å…æ¨¡å‹éåº¦é—œæ³¨\")\n",
    "\n",
    "# è¦–è¦ºåŒ–æ¬Šé‡åˆ†ä½ˆ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. æ¬Šé‡æ¢å½¢åœ–\n",
    "ax1 = axes[0]\n",
    "labels_str = [str(l) for l in unique_labels]\n",
    "weight_values = [class_weights[l] for l in unique_labels]\n",
    "colors_map = ['red' if w > 5 else 'orange' if w > 2 else 'green' for w in weight_values]\n",
    "\n",
    "bars = ax1.bar(labels_str, weight_values, color=colors_map, edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('é¡åˆ¥ ID', fontsize=12)\n",
    "ax1.set_ylabel('æ­£è¦åŒ–æ¬Šé‡', fontsize=12)\n",
    "ax1.set_title('å„é¡åˆ¥æ¬Šé‡åˆ†ä½ˆ', fontsize=14, fontweight='bold')\n",
    "ax1.axhline(y=1.0, color='blue', linestyle='--', linewidth=2, label='åŸºæº–ç·š (1.0)')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax1.legend()\n",
    "\n",
    "# åœ¨æ¢å½¢ä¸Šé¡¯ç¤ºæ•¸å€¼\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. åƒç´ æ•¸ vs æ¬Šé‡æ•£é»åœ–\n",
    "ax2 = axes[1]\n",
    "pixel_values = [pixel_counts[l] for l in unique_labels]\n",
    "weight_values = [class_weights[l] for l in unique_labels]\n",
    "\n",
    "scatter = ax2.scatter(pixel_values, weight_values, s=100, c=weight_values,\n",
    "                     cmap='RdYlGn_r', edgecolors='black', alpha=0.7)\n",
    "ax2.set_xlabel('é¡åˆ¥åƒç´ ç¸½æ•¸', fontsize=12)\n",
    "ax2.set_ylabel('æ­£è¦åŒ–æ¬Šé‡', fontsize=12)\n",
    "ax2.set_title('åƒç´ æ•¸é‡ vs æ¬Šé‡é—œä¿‚', fontsize=14, fontweight='bold')\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# æ·»åŠ æ¨™è¨»\n",
    "for i, label in enumerate(unique_labels):\n",
    "    ax2.annotate(str(label), (pixel_values[i], weight_values[i]),\n",
    "                fontsize=9, ha='center')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax2, label='æ¬Šé‡å€¼')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# è½‰æ›ç‚º TensorFlow å¯ç”¨çš„æ ¼å¼\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”§ è½‰æ›ç‚ºè¨“ç·´ç”¨æ ¼å¼...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å»ºç«‹é¡åˆ¥ç´¢å¼•åˆ°æ¬Šé‡çš„æ˜ å°„ï¼ˆ0-based indexï¼‰\n",
    "class_weight_dict = {}\n",
    "for i, label in enumerate(unique_labels):\n",
    "    class_weight_dict[i] = class_weights[label]\n",
    "\n",
    "print(f\"\\nâœ… é¡åˆ¥æ¬Šé‡å­—å…¸ï¼ˆç”¨æ–¼è¨“ç·´ï¼‰:\")\n",
    "print(f\"  {class_weight_dict}\")\n",
    "\n",
    "print(\"\\nğŸ“ ä½¿ç”¨æ–¹å¼:\")\n",
    "print(\"  åœ¨ model.fit() ä¸­åŠ å…¥åƒæ•¸:\")\n",
    "print(\"  class_weight=class_weight_dict\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… é¡åˆ¥æ¬Šé‡è¨ˆç®—å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-10-18T02:59:40.059808Z",
     "iopub.status.idle": "2025-10-18T02:59:40.059996Z",
     "shell.execute_reply": "2025-10-18T02:59:40.059909Z",
     "shell.execute_reply.started": "2025-10-18T02:59:40.059900Z"
    },
    "id": "84yXbfm-nA0w",
    "outputId": "7540f4a7-f063-48cc-fb8f-39ef38c9fa89",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# è¨ˆç®—è³‡æ–™é›†çµ±è¨ˆé‡\n",
    "print(\"ğŸ“ è¨ˆç®—è³‡æ–™é›†çš„ RGB é€šé“çµ±è¨ˆé‡...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_dataset_statistics(image_paths, sample_size=1000):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—è³‡æ–™é›†çš„å‡å€¼å’Œæ¨™æº–å·®\n",
    "\n",
    "    ç”¨é€”ï¼šæ¯”æ¨™æº–çš„ [-1, 1] æ­£è¦åŒ–æ›´ç²¾ç¢º\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”¬ æŠ½æ¨£ {sample_size} å¼µå½±åƒè¨ˆç®—çµ±è¨ˆé‡...\")\n",
    "\n",
    "    # æŠ½æ¨£\n",
    "    sampled_paths = random.sample(image_paths, min(sample_size, len(image_paths)))\n",
    "\n",
    "    # æ”¶é›†åƒç´ å€¼\n",
    "    r_values = []\n",
    "    g_values = []\n",
    "    b_values = []\n",
    "\n",
    "    for img_path in tqdm(sampled_paths, desc=\"Computing statistics\"):\n",
    "        img = np.array(Image.open(img_path).convert('RGB')) / 255.0  # æ­£è¦åŒ–åˆ° [0, 1]\n",
    "\n",
    "        r_values.append(img[:, :, 0].flatten())\n",
    "        g_values.append(img[:, :, 1].flatten())\n",
    "        b_values.append(img[:, :, 2].flatten())\n",
    "\n",
    "    # åˆä½µæ‰€æœ‰åƒç´ \n",
    "    r_all = np.concatenate(r_values)\n",
    "    g_all = np.concatenate(g_values)\n",
    "    b_all = np.concatenate(b_values)\n",
    "\n",
    "    # è¨ˆç®—å‡å€¼å’Œæ¨™æº–å·®\n",
    "    mean = np.array([r_all.mean(), g_all.mean(), b_all.mean()])\n",
    "    std = np.array([r_all.std(), g_all.std(), b_all.std()])\n",
    "\n",
    "    return mean, std, (r_all, g_all, b_all)\n",
    "\n",
    "# è¨ˆç®—çµ±è¨ˆé‡\n",
    "dataset_mean, dataset_std, channel_data = compute_dataset_statistics(\n",
    "    final_images, sample_size=1000\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š è³‡æ–™é›†çµ±è¨ˆçµæœ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ¨ RGB é€šé“å‡å€¼:\")\n",
    "print(f\"  R (ç´…è‰²): {dataset_mean[0]:.4f}\")\n",
    "print(f\"  G (ç¶ è‰²): {dataset_mean[1]:.4f}\")\n",
    "print(f\"  B (è—è‰²): {dataset_mean[2]:.4f}\")\n",
    "print(f\"  å‘é‡å½¢å¼: {dataset_mean}\")\n",
    "\n",
    "print(f\"\\nğŸ“ RGB é€šé“æ¨™æº–å·®:\")\n",
    "print(f\"  R (ç´…è‰²): {dataset_std[0]:.4f}\")\n",
    "print(f\"  G (ç¶ è‰²): {dataset_std[1]:.4f}\")\n",
    "print(f\"  B (è—è‰²): {dataset_std[2]:.4f}\")\n",
    "print(f\"  å‘é‡å½¢å¼: {dataset_std}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ æ­£è¦åŒ–å…¬å¼:\")\n",
    "print(f\"  normalized_pixel = (pixel / 255.0 - mean) / std\")\n",
    "print(f\"  å³: (pixel / 255.0 - {dataset_mean}) / {dataset_std}\")\n",
    "\n",
    "# è¦–è¦ºåŒ–é€šé“åˆ†ä½ˆ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "r_data, g_data, b_data = channel_data\n",
    "\n",
    "# 1. RGB ç›´æ–¹åœ–\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(r_data, bins=100, color='red', alpha=0.5, label='R', density=True)\n",
    "ax1.hist(g_data, bins=100, color='green', alpha=0.5, label='G', density=True)\n",
    "ax1.hist(b_data, bins=100, color='blue', alpha=0.5, label='B', density=True)\n",
    "ax1.set_xlabel('åƒç´ å€¼ (0-1)', fontsize=12)\n",
    "ax1.set_ylabel('å¯†åº¦', fontsize=12)\n",
    "ax1.set_title('RGB é€šé“åƒç´ å€¼åˆ†ä½ˆ', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. é€šé“çµ±è¨ˆæ¯”è¼ƒ\n",
    "ax2 = axes[0, 1]\n",
    "channels = ['R', 'G', 'B']\n",
    "means = dataset_mean\n",
    "stds = dataset_std\n",
    "\n",
    "x = np.arange(len(channels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, means, width, label='å‡å€¼', color='skyblue', edgecolor='black')\n",
    "bars2 = ax2.bar(x + width/2, stds, width, label='æ¨™æº–å·®', color='lightcoral', edgecolor='black')\n",
    "\n",
    "ax2.set_ylabel('æ•¸å€¼', fontsize=12)\n",
    "ax2.set_title('å„é€šé“çµ±è¨ˆé‡æ¯”è¼ƒ', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(channels)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# åœ¨æ¢å½¢ä¸Šé¡¯ç¤ºæ•¸å€¼\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 3. æ­£è¦åŒ–å‰å¾Œå°æ¯”ï¼ˆç¤ºä¾‹å½±åƒï¼‰\n",
    "ax3 = axes[1, 0]\n",
    "sample_img = np.array(Image.open(final_images[0]).convert('RGB'))\n",
    "ax3.imshow(sample_img)\n",
    "ax3.set_title('åŸå§‹å½±åƒ', fontsize=12, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "# 4. æ­£è¦åŒ–å¾Œçš„å½±åƒ\n",
    "ax4 = axes[1, 1]\n",
    "normalized_img = (sample_img / 255.0 - dataset_mean) / dataset_std\n",
    "# ç‚ºäº†é¡¯ç¤ºï¼Œéœ€è¦åæ­£è¦åŒ–åˆ°å¯è¦–ç¯„åœ\n",
    "display_img = (normalized_img - normalized_img.min()) / (normalized_img.max() - normalized_img.min())\n",
    "ax4.imshow(display_img)\n",
    "ax4.set_title('æ­£è¦åŒ–å¾Œå½±åƒï¼ˆé‡æ–°ç¸®æ”¾ä»¥é¡¯ç¤ºï¼‰', fontsize=12, fontweight='bold')\n",
    "ax4.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ¯”è¼ƒä¸åŒæ­£è¦åŒ–æ–¹æ³•\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” æ­£è¦åŒ–æ–¹æ³•æ¯”è¼ƒ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# æ–¹æ³•1: [-1, 1] æ­£è¦åŒ–\n",
    "method1_mean = [0.0, 0.0, 0.0]\n",
    "method1_std = [1.0, 1.0, 1.0]\n",
    "\n",
    "# æ–¹æ³•2: ImageNet æ¨™æº–å€¼\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# æ–¹æ³•3: è³‡æ–™é›†çµ±è¨ˆå€¼\n",
    "dataset_mean_list = dataset_mean.tolist()\n",
    "dataset_std_list = dataset_std.tolist()\n",
    "\n",
    "comparison_data.append({\n",
    "    'æ–¹æ³•': 'æ¨™æº–æ­£è¦åŒ– [-1,1]',\n",
    "    'Rå‡å€¼': 0.0, 'Gå‡å€¼': 0.0, 'Bå‡å€¼': 0.0,\n",
    "    'Ræ¨™æº–å·®': 1.0, 'Gæ¨™æº–å·®': 1.0, 'Bæ¨™æº–å·®': 1.0,\n",
    "    'å„ªé»': 'ç°¡å–®å¿«é€Ÿ',\n",
    "    'ç¼ºé»': 'ä¸è€ƒæ…®è³‡æ–™åˆ†ä½ˆ'\n",
    "})\n",
    "\n",
    "comparison_data.append({\n",
    "    'æ–¹æ³•': 'ImageNet é è¨“ç·´',\n",
    "    'Rå‡å€¼': imagenet_mean[0], 'Gå‡å€¼': imagenet_mean[1], 'Bå‡å€¼': imagenet_mean[2],\n",
    "    'Ræ¨™æº–å·®': imagenet_std[0], 'Gæ¨™æº–å·®': imagenet_std[1], 'Bæ¨™æº–å·®': imagenet_std[2],\n",
    "    'å„ªé»': 'é©ç”¨æ–¼é·ç§»å­¸ç¿’',\n",
    "    'ç¼ºé»': 'å¯èƒ½ä¸é©åˆæ™‚å°šè³‡æ–™'\n",
    "})\n",
    "\n",
    "comparison_data.append({\n",
    "    'æ–¹æ³•': 'è³‡æ–™é›†è‡ªèº«çµ±è¨ˆ â­',\n",
    "    'Rå‡å€¼': f\"{dataset_mean_list[0]:.4f}\",\n",
    "    'Gå‡å€¼': f\"{dataset_mean_list[1]:.4f}\",\n",
    "    'Bå‡å€¼': f\"{dataset_mean_list[2]:.4f}\",\n",
    "    'Ræ¨™æº–å·®': f\"{dataset_std_list[0]:.4f}\",\n",
    "    'Gæ¨™æº–å·®': f\"{dataset_std_list[1]:.4f}\",\n",
    "    'Bæ¨™æº–å·®': f\"{dataset_std_list[2]:.4f}\",\n",
    "    'å„ªé»': 'æœ€ç²¾ç¢º',\n",
    "    'ç¼ºé»': 'éœ€è¦è¨ˆç®—æ™‚é–“'\n",
    "})\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\nâœ… æ¨è–¦ä½¿ç”¨: è³‡æ–™é›†è‡ªèº«çµ±è¨ˆå€¼ï¼ˆå·²è¨ˆç®—ï¼‰\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… çµ±è¨ˆé‡è¨ˆç®—å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T02:59:40.061408Z",
     "iopub.status.idle": "2025-10-18T02:59:40.061652Z",
     "shell.execute_reply": "2025-10-18T02:59:40.061539Z",
     "shell.execute_reply.started": "2025-10-18T02:59:40.061527Z"
    },
    "id": "TYV6dm6fzf2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# è³‡æ–™åˆ†å‰²ï¼ˆåˆ†å±¤åˆ†å‰²ï¼‰\n",
    "\n",
    "print(\"âœ‚ï¸ åˆ†å‰²è³‡æ–™é›†ç‚ºè¨“ç·´/é©—è­‰/æ¸¬è©¦é›†...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def stratified_split(image_paths, mask_paths, val_size=0.1, test_size=0.05):\n",
    "    \"\"\"\n",
    "    åˆ†å±¤åˆ†å‰²è³‡æ–™é›†\n",
    "\n",
    "    ç­–ç•¥ï¼š\n",
    "    1. æ ¹æ“šé®ç½©ä¸­çš„ä¸»è¦é¡åˆ¥é€²è¡Œåˆ†å±¤\n",
    "    2. ç¢ºä¿æ¯å€‹é¡åˆ¥åœ¨å„é›†åˆä¸­éƒ½æœ‰ä»£è¡¨\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ” åˆ†ææ¯å€‹æ¨£æœ¬çš„ä¸»è¦é¡åˆ¥...\")\n",
    "\n",
    "    dominant_classes = []\n",
    "\n",
    "    for mask_path in tqdm(mask_paths, desc=\"Analyzing dominant classes\"):\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        unique, counts = np.unique(mask, return_counts=True)\n",
    "\n",
    "        # æ‰¾å‡ºæœ€å¤šåƒç´ çš„é¡åˆ¥ï¼ˆæ’é™¤èƒŒæ™¯0ï¼‰\n",
    "        non_bg = [(u, c) for u, c in zip(unique, counts) if u > 0]\n",
    "        if non_bg:\n",
    "            dominant_class = max(non_bg, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            dominant_class = 0\n",
    "\n",
    "        dominant_classes.append(dominant_class)\n",
    "\n",
    "    print(f\"\\næ‰¾åˆ° {len(set(dominant_classes))} å€‹ä¸åŒçš„ä¸»è¦é¡åˆ¥\")\n",
    "\n",
    "    # ç¬¬ä¸€æ¬¡åˆ†å‰²: è¨“ç·´é›† vs (é©—è­‰+æ¸¬è©¦)é›†\n",
    "    print(f\"\\nç¬¬ä¸€æ¬¡åˆ†å‰²: {(1-val_size)*100:.0f}% è¨“ç·´, {val_size*100:.0f}% (é©—è­‰+æ¸¬è©¦)...\")\n",
    "\n",
    "    train_images, temp_images, train_masks, temp_masks, train_classes, temp_classes = \\\n",
    "        train_test_split(\n",
    "            image_paths, mask_paths, dominant_classes,\n",
    "            test_size=val_size,\n",
    "            stratify=dominant_classes,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    # ç¬¬äºŒæ¬¡åˆ†å‰²: é©—è­‰é›† vs æ¸¬è©¦é›†\n",
    "    relative_test_size = test_size / val_size\n",
    "    print(f\"ç¬¬äºŒæ¬¡åˆ†å‰²: {(1-relative_test_size)*100:.0f}% é©—è­‰, {relative_test_size*100:.0f}% æ¸¬è©¦...\")\n",
    "\n",
    "    val_images, test_images, val_masks, test_masks = \\\n",
    "        train_test_split(\n",
    "            temp_images, temp_masks,\n",
    "            test_size=relative_test_size,\n",
    "            stratify=temp_classes,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    return (train_images, val_images, test_images,\n",
    "            train_masks, val_masks, test_masks,\n",
    "            train_classes, temp_classes)\n",
    "\n",
    "# åŸ·è¡Œåˆ†å‰²\n",
    "(train_images, val_images, test_images,\n",
    " train_masks, val_masks, test_masks,\n",
    " train_classes, temp_classes) = stratified_split(\n",
    "    final_images, final_masks, val_size=0.1, test_size=0.05\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºåˆ†å‰²çµæœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š è³‡æ–™åˆ†å‰²çµæœ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total = len(final_images)\n",
    "train_size = len(train_images)\n",
    "val_size = len(val_images)\n",
    "test_size = len(test_images)\n",
    "\n",
    "print(f\"\\nç¸½è³‡æ–™é‡: {total} å°\")\n",
    "print(f\"\\nè¨“ç·´é›†: {train_size} å° ({train_size/total*100:.1f}%)\")\n",
    "print(f\"é©—è­‰é›†: {val_size} å° ({val_size/total*100:.1f}%)\")\n",
    "print(f\"æ¸¬è©¦é›†: {test_size} å° ({test_size/total*100:.1f}%)\")\n",
    "\n",
    "# æª¢æŸ¥é¡åˆ¥åˆ†ä½ˆ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” æª¢æŸ¥å„é›†åˆçš„é¡åˆ¥åˆ†ä½ˆ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æ”¶é›†æ¯å€‹é›†åˆçš„é¡åˆ¥åˆ†ä½ˆ\n",
    "def get_class_distribution(mask_paths, set_name):\n",
    "    \"\"\"çµ±è¨ˆé¡åˆ¥åˆ†ä½ˆ\"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    for mask_path in mask_paths[:100]:  # æŠ½æ¨£100å¼µæª¢æŸ¥\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        unique = np.unique(mask)\n",
    "        for u in unique:\n",
    "            class_counts[int(u)] += 1\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "train_dist = get_class_distribution(train_masks, \"è¨“ç·´é›†\")\n",
    "val_dist = get_class_distribution(val_masks, \"é©—è­‰é›†\")\n",
    "test_dist = get_class_distribution(test_masks, \"æ¸¬è©¦é›†\")\n",
    "\n",
    "print(\"\\nå„é¡åˆ¥åœ¨ä¸åŒé›†åˆä¸­çš„å‡ºç¾æ¬¡æ•¸ï¼ˆæŠ½æ¨£100å¼µï¼‰:\")\n",
    "dist_data = []\n",
    "for label in sorted(unique_labels):\n",
    "    dist_data.append({\n",
    "        'é¡åˆ¥': label,\n",
    "        'è¨“ç·´é›†': train_dist.get(label, 0),\n",
    "        'é©—è­‰é›†': val_dist.get(label, 0),\n",
    "        'æ¸¬è©¦é›†': test_dist.get(label, 0)\n",
    "    })\n",
    "\n",
    "df_dist = pd.DataFrame(dist_data)\n",
    "print(df_dist.to_string(index=False))\n",
    "\n",
    "# è¦–è¦ºåŒ–åˆ†å‰²çµæœ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. è³‡æ–™é›†å¤§å°æ¯”è¼ƒ\n",
    "ax1 = axes[0, 0]\n",
    "sets = ['è¨“ç·´é›†', 'é©—è­‰é›†', 'æ¸¬è©¦é›†']\n",
    "sizes = [train_size, val_size, test_size]\n",
    "colors_set = ['#4CAF50', '#2196F3', '#FF9800']\n",
    "\n",
    "bars = ax1.bar(sets, sizes, color=colors_set, edgecolor='black', alpha=0.7)\n",
    "ax1.set_ylabel('æ¨£æœ¬æ•¸é‡', fontsize=12)\n",
    "ax1.set_title('è³‡æ–™é›†åˆ†å‰²æ¯”ä¾‹', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    percentage = (height / total) * 100\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}\\n({percentage:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 2. åœ“é¤…åœ–\n",
    "ax2 = axes[0, 1]\n",
    "ax2.pie(sizes, labels=sets, colors=colors_set, autopct='%1.1f%%',\n",
    "        startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "ax2.set_title('è³‡æ–™åˆ†ä½ˆæ¯”ä¾‹', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. é¡åˆ¥åˆ†ä½ˆæ¯”è¼ƒï¼ˆå †ç–ŠæŸ±ç‹€åœ–ï¼‰\n",
    "ax3 = axes[1, 0]\n",
    "labels_for_plot = [str(l) for l in sorted(unique_labels)[:10]]  # åªé¡¯ç¤ºå‰10å€‹é¡åˆ¥\n",
    "train_counts = [train_dist.get(l, 0) for l in sorted(unique_labels)[:10]]\n",
    "val_counts = [val_dist.get(l, 0) for l in sorted(unique_labels)[:10]]\n",
    "test_counts = [test_dist.get(l, 0) for l in sorted(unique_labels)[:10]]\n",
    "\n",
    "x = np.arange(len(labels_for_plot))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax3.bar(x - width, train_counts, width, label='è¨“ç·´é›†', color='#4CAF50', alpha=0.7)\n",
    "bars2 = ax3.bar(x, val_counts, width, label='é©—è­‰é›†', color='#2196F3', alpha=0.7)\n",
    "bars3 = ax3.bar(x + width, test_counts, width, label='æ¸¬è©¦é›†', color='#FF9800', alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('é¡åˆ¥ ID', fontsize=12)\n",
    "ax3.set_ylabel('å‡ºç¾æ¬¡æ•¸ï¼ˆæŠ½æ¨£ï¼‰', fontsize=12)\n",
    "ax3.set_title('å„é›†åˆé¡åˆ¥åˆ†ä½ˆæ¯”è¼ƒï¼ˆå‰10é¡åˆ¥ï¼‰', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(labels_for_plot)\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. é¡¯ç¤ºç¯„ä¾‹æ¨£æœ¬\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "info_text = f\"\"\"\n",
    "ğŸ“Š åˆ†å‰²çµ±è¨ˆæ‘˜è¦\n",
    "\n",
    "ç¸½è³‡æ–™é‡: {total:,} å°\n",
    "\n",
    "è¨“ç·´é›†: {train_size:,} ({train_size/total*100:.1f}%)\n",
    "é©—è­‰é›†: {val_size:,} ({val_size/total*100:.1f}%)\n",
    "æ¸¬è©¦é›†: {test_size:,} ({test_size/total*100:.1f}%)\n",
    "\n",
    "âœ… ä½¿ç”¨åˆ†å±¤åˆ†å‰²ç­–ç•¥\n",
    "âœ… æ‰€æœ‰é¡åˆ¥å‡æœ‰ä»£è¡¨\n",
    "âœ… éš¨æ©Ÿç¨®å­: {RANDOM_STATE}\n",
    "\"\"\"\n",
    "ax4.text(0.1, 0.5, info_text, fontsize=12, verticalalignment='center',\n",
    "         family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# é©—è­‰åˆ†å‰²çš„åˆç†æ€§\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… åˆ†å‰²åˆç†æ€§æª¢æŸ¥:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æª¢æŸ¥1: ç¢ºä¿æ²’æœ‰é‡è¤‡\n",
    "train_set = set(train_images)\n",
    "val_set = set(val_images)\n",
    "test_set = set(test_images)\n",
    "\n",
    "overlap_train_val = train_set & val_set\n",
    "overlap_train_test = train_set & test_set\n",
    "overlap_val_test = val_set & test_set\n",
    "\n",
    "print(f\"\\né‡è¤‡æª¢æŸ¥:\")\n",
    "print(f\"  è¨“ç·´é›† âˆ© é©—è­‰é›†: {len(overlap_train_val)} (æ‡‰ç‚º 0)\")\n",
    "print(f\"  è¨“ç·´é›† âˆ© æ¸¬è©¦é›†: {len(overlap_train_test)} (æ‡‰ç‚º 0)\")\n",
    "print(f\"  é©—è­‰é›† âˆ© æ¸¬è©¦é›†: {len(overlap_val_test)} (æ‡‰ç‚º 0)\")\n",
    "\n",
    "if len(overlap_train_val) == 0 and len(overlap_train_test) == 0 and len(overlap_val_test) == 0:\n",
    "    print(\"  âœ… é€šéï¼šæ²’æœ‰é‡è¤‡æ¨£æœ¬\")\n",
    "else:\n",
    "    print(\"  âŒ è­¦å‘Šï¼šç™¼ç¾é‡è¤‡æ¨£æœ¬ï¼\")\n",
    "\n",
    "# æª¢æŸ¥2: ç¸½æ•¸æ˜¯å¦æ­£ç¢º\n",
    "total_split = len(train_images) + len(val_images) + len(test_images)\n",
    "print(f\"\\nç¸½æ•¸æª¢æŸ¥:\")\n",
    "print(f\"  åˆ†å‰²å‰: {total}\")\n",
    "print(f\"  åˆ†å‰²å¾Œ: {total_split}\")\n",
    "print(f\"  å·®ç•°: {total - total_split}\")\n",
    "\n",
    "if total == total_split:\n",
    "    print(\"  âœ… é€šéï¼šç¸½æ•¸ä¸€è‡´\")\n",
    "else:\n",
    "    print(\"  âŒ è­¦å‘Šï¼šç¸½æ•¸ä¸ä¸€è‡´ï¼\")\n",
    "\n",
    "# é¡¯ç¤ºæ¨£æœ¬ç¯„ä¾‹\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ æ¨£æœ¬è·¯å¾‘ç¯„ä¾‹:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nè¨“ç·´é›†ï¼ˆå‰3å€‹ï¼‰:\")\n",
    "for i in range(min(3, len(train_images))):\n",
    "    print(f\"  {i+1}. å½±åƒ: {os.path.basename(train_images[i])}\")\n",
    "    print(f\"     é®ç½©: {os.path.basename(train_masks[i])}\")\n",
    "\n",
    "print(\"\\né©—è­‰é›†ï¼ˆå‰3å€‹ï¼‰:\")\n",
    "for i in range(min(3, len(val_images))):\n",
    "    print(f\"  {i+1}. å½±åƒ: {os.path.basename(val_images[i])}\")\n",
    "    print(f\"     é®ç½©: {os.path.basename(val_masks[i])}\")\n",
    "\n",
    "print(\"\\næ¸¬è©¦é›†ï¼ˆå‰3å€‹ï¼‰:\")\n",
    "for i in range(min(3, len(test_images))):\n",
    "    print(f\"  {i+1}. å½±åƒ: {os.path.basename(test_images[i])}\")\n",
    "    print(f\"     é®ç½©: {os.path.basename(test_masks[i])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… è³‡æ–™åˆ†å‰²å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T02:59:40.062706Z",
     "iopub.status.idle": "2025-10-18T02:59:40.062990Z",
     "shell.execute_reply": "2025-10-18T02:59:40.062858Z",
     "shell.execute_reply.started": "2025-10-18T02:59:40.062846Z"
    },
    "id": "WIDWO22OzrBy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 10: å½±åƒè®€å–èˆ‡æ­£è¦åŒ–å‡½æ•¸\n",
    "\n",
    "print(\"ğŸ–¼ï¸ å»ºç«‹å½±åƒè®€å–èˆ‡æ­£è¦åŒ–å‡½æ•¸...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def read_and_normalize_image(image_path, mask_path):\n",
    "    \"\"\"\n",
    "    è®€å–ä¸¦æ­£è¦åŒ–å½±åƒå’Œé®ç½©\n",
    "\n",
    "    å½±åƒè™•ç†:\n",
    "    1. è®€å– JPEG (RGB 3é€šé“)\n",
    "    2. ç¸®æ”¾åˆ° 512x512\n",
    "    3. ä½¿ç”¨è³‡æ–™é›†çµ±è¨ˆé‡æ­£è¦åŒ–\n",
    "\n",
    "    é®ç½©è™•ç†:\n",
    "    1. è®€å– PNG (å–®é€šé“)\n",
    "    2. ç¸®æ”¾åˆ° 512x512 (æœ€è¿‘é„°æ’å€¼)\n",
    "    3. ä¿æŒåŸå§‹æ¨™ç±¤å€¼\n",
    "    \"\"\"\n",
    "    # è®€å–å½±åƒ\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "\n",
    "    # ç¸®æ”¾å½±åƒ\n",
    "    img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE], method='bilinear')\n",
    "\n",
    "    # æ­£è¦åŒ–å½±åƒ: (pixel / 255.0 - mean) / std\n",
    "    img = img / 255.0\n",
    "    img = (img - dataset_mean) / dataset_std\n",
    "\n",
    "    # è®€å–é®ç½©\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=1)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # ç¸®æ”¾é®ç½© (ä½¿ç”¨æœ€è¿‘é„°æ’å€¼ä¿æŒæ¨™ç±¤å®Œæ•´æ€§)\n",
    "    mask = tf.image.resize(mask, [IMAGE_SIZE, IMAGE_SIZE], method='nearest')\n",
    "\n",
    "    # å°‡é®ç½©å€¼è½‰æ›ç‚ºé¡åˆ¥ç´¢å¼• (0 åˆ° NUM_CLASSES-1)\n",
    "    mask = mask / 255.0  # æ­£è¦åŒ–åˆ° [0, 1]\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "# æ¸¬è©¦è®€å–å‡½æ•¸\n",
    "print(\"\\nğŸ§ª æ¸¬è©¦å½±åƒè®€å–å‡½æ•¸...\")\n",
    "\n",
    "# è®€å–ä¸€å€‹æ¨£æœ¬\n",
    "test_img, test_mask = read_and_normalize_image(\n",
    "    train_images[0], train_masks[0]\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… è®€å–æˆåŠŸï¼\")\n",
    "print(f\"  å½±åƒå½¢ç‹€: {test_img.shape}\")\n",
    "print(f\"  å½±åƒæ•¸å€¼ç¯„åœ: [{test_img.numpy().min():.3f}, {test_img.numpy().max():.3f}]\")\n",
    "print(f\"  å½±åƒæ•¸æ“šé¡å‹: {test_img.dtype}\")\n",
    "\n",
    "print(f\"\\n  é®ç½©å½¢ç‹€: {test_mask.shape}\")\n",
    "print(f\"  é®ç½©æ•¸å€¼ç¯„åœ: [{test_mask.numpy().min():.3f}, {test_mask.numpy().max():.3f}]\")\n",
    "print(f\"  é®ç½©å”¯ä¸€å€¼: {np.unique(test_mask.numpy())[:10]}\")  # é¡¯ç¤ºå‰10å€‹\n",
    "print(f\"  é®ç½©æ•¸æ“šé¡å‹: {test_mask.dtype}\")\n",
    "\n",
    "# è¦–è¦ºåŒ–è™•ç†å¾Œçš„æ¨£æœ¬\n",
    "print(\"\\nğŸ“Š è¦–è¦ºåŒ–è™•ç†å¾Œçš„è³‡æ–™...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# è™•ç†3å€‹æ¨£æœ¬\n",
    "for i in range(3):\n",
    "    img_processed, mask_processed = read_and_normalize_image(\n",
    "        train_images[i], train_masks[i]\n",
    "    )\n",
    "\n",
    "    # åŸå§‹å½±åƒï¼ˆåæ­£è¦åŒ–ä»¥é¡¯ç¤ºï¼‰\n",
    "    img_display = img_processed.numpy() * dataset_std + dataset_mean\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "    # é¡¯ç¤ºå½±åƒ\n",
    "    axes[0, i].imshow(img_display)\n",
    "    axes[0, i].set_title(f'æ¨£æœ¬ {i+1} - è™•ç†å¾Œå½±åƒ\\n{IMAGE_SIZE}x{IMAGE_SIZE}',\n",
    "                        fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # é¡¯ç¤ºé®ç½©\n",
    "    im = axes[1, i].imshow(mask_processed.numpy()[:, :, 0], cmap='jet', vmin=0, vmax=1)\n",
    "    axes[1, i].set_title(f'æ¨£æœ¬ {i+1} - è™•ç†å¾Œé®ç½©\\n{IMAGE_SIZE}x{IMAGE_SIZE}',\n",
    "                        fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1, i], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# é¡¯ç¤ºæ­£è¦åŒ–æ•ˆæœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ˆ æ­£è¦åŒ–æ•ˆæœåˆ†æ:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æ”¶é›†å¤šå€‹æ¨£æœ¬çš„çµ±è¨ˆè³‡è¨Š\n",
    "sample_means = []\n",
    "sample_stds = []\n",
    "\n",
    "for i in range(min(10, len(train_images))):\n",
    "    img, _ = read_and_normalize_image(train_images[i], train_masks[i])\n",
    "    img_np = img.numpy()\n",
    "    sample_means.append([img_np[:,:,0].mean(), img_np[:,:,1].mean(), img_np[:,:,2].mean()])\n",
    "    sample_stds.append([img_np[:,:,0].std(), img_np[:,:,1].std(), img_np[:,:,2].std()])\n",
    "\n",
    "sample_means = np.array(sample_means)\n",
    "sample_stds = np.array(sample_stds)\n",
    "\n",
    "print(f\"\\næ­£è¦åŒ–å¾Œçš„çµ±è¨ˆï¼ˆ10å€‹æ¨£æœ¬ï¼‰:\")\n",
    "print(f\"\\nRGB å‡å€¼å¹³å‡:\")\n",
    "print(f\"  R: {sample_means[:,0].mean():.4f} Â± {sample_means[:,0].std():.4f}\")\n",
    "print(f\"  G: {sample_means[:,1].mean():.4f} Â± {sample_means[:,1].std():.4f}\")\n",
    "print(f\"  B: {sample_means[:,2].mean():.4f} Â± {sample_means[:,2].std():.4f}\")\n",
    "\n",
    "print(f\"\\nRGB æ¨™æº–å·®å¹³å‡:\")\n",
    "print(f\"  R: {sample_stds[:,0].mean():.4f} Â± {sample_stds[:,0].std():.4f}\")\n",
    "print(f\"  G: {sample_stds[:,1].mean():.4f} Â± {sample_stds[:,1].std():.4f}\")\n",
    "print(f\"  B: {sample_stds[:,2].mean():.4f} Â± {sample_stds[:,2].std():.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ç†æƒ³æƒ…æ³ï¼šæ­£è¦åŒ–å¾Œå‡å€¼æ‡‰æ¥è¿‘ 0ï¼Œæ¨™æº–å·®æ‡‰æ¥è¿‘ 1\")\n",
    "\n",
    "# è¦–è¦ºåŒ–æ­£è¦åŒ–æ•ˆæœ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# å‡å€¼åˆ†ä½ˆ\n",
    "ax1 = axes[0]\n",
    "ax1.boxplot([sample_means[:,0], sample_means[:,1], sample_means[:,2]],\n",
    "            labels=['R', 'G', 'B'])\n",
    "ax1.axhline(y=0, color='r', linestyle='--', linewidth=2, label='ç›®æ¨™å‡å€¼ (0)')\n",
    "ax1.set_ylabel('å‡å€¼', fontsize=12)\n",
    "ax1.set_title('æ­£è¦åŒ–å¾Œå„é€šé“å‡å€¼åˆ†ä½ˆ', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# æ¨™æº–å·®åˆ†ä½ˆ\n",
    "ax2 = axes[1]\n",
    "ax2.boxplot([sample_stds[:,0], sample_stds[:,1], sample_stds[:,2]],\n",
    "            labels=['R', 'G', 'B'])\n",
    "ax2.axhline(y=1, color='r', linestyle='--', linewidth=2, label='ç›®æ¨™æ¨™æº–å·® (1)')\n",
    "ax2.set_ylabel('æ¨™æº–å·®', fontsize=12)\n",
    "ax2.set_title('æ­£è¦åŒ–å¾Œå„é€šé“æ¨™æº–å·®åˆ†ä½ˆ', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… å½±åƒè®€å–èˆ‡æ­£è¦åŒ–å‡½æ•¸å»ºç«‹å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T02:59:40.063480Z",
     "iopub.status.idle": "2025-10-18T02:59:40.064454Z",
     "shell.execute_reply": "2025-10-18T02:59:40.064265Z",
     "shell.execute_reply.started": "2025-10-18T02:59:40.064218Z"
    },
    "id": "N-PAOq96zt9y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  è³‡æ–™å¢å¼·å‡½æ•¸ \n",
    "\n",
    "print(\"ğŸ¨ å»ºç«‹è³‡æ–™å¢å¼·å‡½æ•¸...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å¢å¼·æ©Ÿç‡è¨­å®š\n",
    "AUGMENTATION_CONFIG = {\n",
    "    'horizontal_flip_prob': 0.5,\n",
    "    'rotation_prob': 0.3,\n",
    "    'brightness_prob': 0.4,\n",
    "    'contrast_prob': 0.4,\n",
    "    'saturation_prob': 0.3,\n",
    "    'random_crop_prob': 0.3,\n",
    "}\n",
    "\n",
    "print(\"\\nâš™ï¸ å¢å¼·é…ç½®:\")\n",
    "for key, value in AUGMENTATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 1. æ°´å¹³ç¿»è½‰\n",
    "@tf.function\n",
    "def random_horizontal_flip(image, mask):\n",
    "    \"\"\"éš¨æ©Ÿæ°´å¹³ç¿»è½‰ï¼ˆå½±åƒå’Œé®ç½©åŒæ­¥ï¼‰\"\"\"\n",
    "    if tf.random.uniform([]) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "    return image, mask\n",
    "\n",
    "# 2. æ—‹è½‰ï¼ˆ90åº¦å€æ•¸ï¼‰\n",
    "@tf.function\n",
    "def random_rotation(image, mask):\n",
    "    \"\"\"éš¨æ©Ÿæ—‹è½‰90åº¦å€æ•¸\"\"\"\n",
    "    if tf.random.uniform([]) < AUGMENTATION_CONFIG['rotation_prob']:\n",
    "        k = tf.random.uniform([], 0, 4, dtype=tf.int32)\n",
    "        image = tf.image.rot90(image, k=k)\n",
    "        mask = tf.image.rot90(mask, k=k)\n",
    "    return image, mask\n",
    "\n",
    "# 3. äº®åº¦èª¿æ•´\n",
    "@tf.function\n",
    "def random_brightness(image, mask):\n",
    "    \"\"\"éš¨æ©Ÿèª¿æ•´äº®åº¦ï¼ˆåªå°å½±åƒï¼‰\"\"\"\n",
    "    if tf.random.uniform([]) < AUGMENTATION_CONFIG['brightness_prob']:\n",
    "        image = tf.image.adjust_brightness(image, tf.random.uniform([], -0.2, 0.2))\n",
    "    return image, mask\n",
    "\n",
    "# 4. å°æ¯”åº¦èª¿æ•´\n",
    "@tf.function\n",
    "def random_contrast(image, mask):\n",
    "    \"\"\"éš¨æ©Ÿèª¿æ•´å°æ¯”åº¦ï¼ˆåªå°å½±åƒï¼‰\"\"\"\n",
    "    if tf.random.uniform([]) < AUGMENTATION_CONFIG['contrast_prob']:\n",
    "        image = tf.image.adjust_contrast(image, tf.random.uniform([], 0.8, 1.2))\n",
    "    return image, mask\n",
    "\n",
    "# 5. é£½å’Œåº¦èª¿æ•´\n",
    "@tf.function\n",
    "def random_saturation(image, mask):\n",
    "    \"\"\"éš¨æ©Ÿèª¿æ•´é£½å’Œåº¦ï¼ˆåªå°å½±åƒï¼‰\"\"\"\n",
    "    if tf.random.uniform([]) < AUGMENTATION_CONFIG['saturation_prob']:\n",
    "        image = tf.image.adjust_saturation(image, tf.random.uniform([], 0.8, 1.2))\n",
    "    return image, mask\n",
    "\n",
    "# 6. éš¨æ©Ÿè£åˆ‡\n",
    "@tf.function\n",
    "def random_crop_and_resize(image, mask):\n",
    "    \"\"\"éš¨æ©Ÿè£åˆ‡ä¸¦ç¸®æ”¾å›åŸå°ºå¯¸\"\"\"\n",
    "    if tf.random.uniform([]) < AUGMENTATION_CONFIG['random_crop_prob']:\n",
    "        # éš¨æ©Ÿè£åˆ‡å¤§å° (80%-100%)\n",
    "        crop_size = tf.random.uniform([], 0.8, 1.0)\n",
    "        h = tf.cast(IMAGE_SIZE * crop_size, tf.int32)\n",
    "        w = tf.cast(IMAGE_SIZE * crop_size, tf.int32)\n",
    "\n",
    "        # åˆä½µå½±åƒå’Œé®ç½©é€²è¡Œç›¸åŒè£åˆ‡\n",
    "        combined = tf.concat([image, mask], axis=-1)\n",
    "        combined = tf.image.random_crop(combined, [h, w, 4])\n",
    "\n",
    "        # ç¸®æ”¾å›åŸå°ºå¯¸\n",
    "        combined = tf.image.resize(combined, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "\n",
    "        image = combined[:, :, :3]\n",
    "        mask = combined[:, :, 3:]\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "# çµ„åˆæ‰€æœ‰å¢å¼·\n",
    "@tf.function\n",
    "def apply_augmentation(image, mask):\n",
    "    \"\"\"æ‡‰ç”¨æ‰€æœ‰å¢å¼·æ–¹æ³•\"\"\"\n",
    "    # å¹¾ä½•è®Šæ›\n",
    "    image, mask = random_horizontal_flip(image, mask)\n",
    "    image, mask = random_rotation(image, mask)\n",
    "    image, mask = random_crop_and_resize(image, mask)\n",
    "\n",
    "    # è‰²å½©å¢å¼·ï¼ˆåªå°å½±åƒï¼‰\n",
    "    image, mask = random_brightness(image, mask)\n",
    "    image, mask = random_contrast(image, mask)\n",
    "    image, mask = random_saturation(image, mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "# æ¸¬è©¦å¢å¼·æ•ˆæœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ§ª æ¸¬è©¦è³‡æ–™å¢å¼·æ•ˆæœ...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# è¼‰å…¥ä¸€å€‹æ¨£æœ¬\n",
    "test_img, test_mask = read_and_normalize_image(train_images[0], train_masks[0])\n",
    "\n",
    "# ç”Ÿæˆå¤šå€‹å¢å¼·ç‰ˆæœ¬\n",
    "print(\"\\nç”Ÿæˆ 8 å€‹å¢å¼·æ¨£æœ¬...\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "for i in range(8):\n",
    "    # æ‡‰ç”¨å¢å¼·\n",
    "    aug_img, aug_mask = apply_augmentation(test_img, test_mask)\n",
    "\n",
    "    # åæ­£è¦åŒ–å½±åƒä»¥é¡¯ç¤º\n",
    "    display_img = aug_img.numpy() * dataset_std + dataset_mean\n",
    "    display_img = np.clip(display_img, 0, 1)\n",
    "\n",
    "    # é¡¯ç¤ºå½±åƒ\n",
    "    row = i // 2\n",
    "    col = (i % 2) * 2\n",
    "    axes[row, col].imshow(display_img)\n",
    "    axes[row, col].set_title(f'å¢å¼·æ¨£æœ¬ {i+1} - å½±åƒ', fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "    # é¡¯ç¤ºé®ç½©\n",
    "    axes[row, col+1].imshow(aug_mask.numpy()[:, :, 0], cmap='jet', vmin=0, vmax=1)\n",
    "    axes[row, col+1].set_title(f'å¢å¼·æ¨£æœ¬ {i+1} - é®ç½©', fontsize=10)\n",
    "    axes[row, col+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# çµ±è¨ˆå¢å¼·æ•ˆæœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š å¢å¼·æ•ˆæœçµ±è¨ˆï¼ˆ100æ¬¡å¢å¼·ï¼‰:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "augmentation_stats = {\n",
    "    'flipped': 0,\n",
    "    'rotated': 0,\n",
    "    'cropped': 0,\n",
    "    'brightness_changed': 0,\n",
    "    'contrast_changed': 0,\n",
    "}\n",
    "\n",
    "original_img, original_mask = read_and_normalize_image(train_images[0], train_masks[0])\n",
    "\n",
    "for _ in tqdm(range(100), desc=\"Testing augmentations\"):\n",
    "    aug_img, aug_mask = apply_augmentation(original_img, original_mask)\n",
    "\n",
    "    # æª¢æ¸¬æ˜¯å¦æœ‰ç¿»è½‰ï¼ˆæ¯”è¼ƒåƒç´ ï¼‰\n",
    "    if not tf.reduce_all(tf.equal(aug_img, original_img)):\n",
    "        # æŸç¨®å¢å¼·è¢«æ‡‰ç”¨äº†\n",
    "        pass\n",
    "\n",
    "print(\"\\nğŸ’¡ å¢å¼·èªªæ˜:\")\n",
    "print(\"  - æ¯æ¬¡å¢å¼·æ™‚ï¼Œå„å€‹æ–¹æ³•æ ¹æ“šè¨­å®šçš„æ©Ÿç‡ç¨ç«‹æ‡‰ç”¨\")\n",
    "print(\"  - å½±åƒå’Œé®ç½©çš„å¹¾ä½•è®Šæ›ä¿æŒåŒæ­¥\")\n",
    "print(\"  - è‰²å½©å¢å¼·åªæ‡‰ç”¨æ–¼å½±åƒï¼Œä¸å½±éŸ¿é®ç½©\")\n",
    "\n",
    "# æ¯”è¼ƒåŸå§‹ vs å¢å¼·\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” åŸå§‹ vs å¢å¼·å°æ¯”:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# åŸå§‹å½±åƒ\n",
    "orig_display = original_img.numpy() * dataset_std + dataset_mean\n",
    "orig_display = np.clip(orig_display, 0, 1)\n",
    "\n",
    "axes[0, 0].imshow(orig_display)\n",
    "axes[0, 0].set_title('åŸå§‹å½±åƒ', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(original_mask.numpy()[:, :, 0], cmap='jet', vmin=0, vmax=1)\n",
    "axes[1, 0].set_title('åŸå§‹é®ç½©', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# 3å€‹ä¸åŒçš„å¢å¼·ç‰ˆæœ¬\n",
    "for i in range(3):\n",
    "    aug_img, aug_mask = apply_augmentation(original_img, original_mask)\n",
    "\n",
    "    aug_display = aug_img.numpy() * dataset_std + dataset_mean\n",
    "    aug_display = np.clip(aug_display, 0, 1)\n",
    "\n",
    "    axes[0, i+1].imshow(aug_display)\n",
    "    axes[0, i+1].set_title(f'å¢å¼·ç‰ˆæœ¬ {i+1}', fontsize=12, fontweight='bold')\n",
    "    axes[0, i+1].axis('off')\n",
    "\n",
    "    axes[1, i+1].imshow(aug_mask.numpy()[:, :, 0], cmap='jet', vmin=0, vmax=1)\n",
    "    axes[1, i+1].set_title(f'å¢å¼·é®ç½© {i+1}', fontsize=12, fontweight='bold')\n",
    "    axes[1, i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… è§€å¯Ÿè¦é»:\")\n",
    "print(\"  âœ“ å½±åƒå’Œé®ç½©çš„å¹¾ä½•è®Šæ›æ˜¯å¦åŒæ­¥ï¼Ÿ\")\n",
    "print(\"  âœ“ é®ç½©çš„æ¨™ç±¤å€¼æ˜¯å¦ä¿æŒå®Œæ•´ï¼ˆç„¡æ’å€¼æ¨¡ç³Šï¼‰ï¼Ÿ\")\n",
    "print(\"  âœ“ è‰²å½©è®ŠåŒ–æ˜¯å¦è‡ªç„¶ï¼Ÿ\")\n",
    "print(\"  âœ“ è£åˆ‡å¾Œçš„å…§å®¹æ˜¯å¦ä»ç„¶æœ‰æ„ç¾©ï¼Ÿ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… è³‡æ–™å¢å¼·å‡½æ•¸å»ºç«‹å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yfJAMcOzwrw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# å»ºç«‹ TensorFlow Dataset ç®¡ç·š \n",
    "print(\"ğŸš€ å»ºç«‹é«˜æ•ˆèƒ½ TensorFlow Dataset ç®¡ç·š...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_dataset(image_paths, mask_paths, is_training=True, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    å»ºç«‹å„ªåŒ–çš„è³‡æ–™ç®¡ç·š\n",
    "\n",
    "    æ­¥é©Ÿ:\n",
    "    1. å¾è·¯å¾‘å»ºç«‹ dataset\n",
    "    2. è®€å–å’Œæ­£è¦åŒ–\n",
    "    3. å¿«å–åˆ°è¨˜æ†¶é«”\n",
    "    4. è³‡æ–™å¢å¼·ï¼ˆåƒ…è¨“ç·´é›†ï¼‰\n",
    "    5. æ‰“äº‚ï¼ˆåƒ…è¨“ç·´é›†ï¼‰\n",
    "    6. æ‰¹æ¬¡åŒ–\n",
    "    7. é å–\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“¦ å»ºç«‹ {'è¨“ç·´' if is_training else 'é©—è­‰/æ¸¬è©¦'} è³‡æ–™é›†...\")\n",
    "    print(f\"  æ¨£æœ¬æ•¸é‡: {len(image_paths)}\")\n",
    "    print(f\"  æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "\n",
    "    # æ­¥é©Ÿ1: å»ºç«‹è·¯å¾‘ dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
    "\n",
    "    # æ­¥é©Ÿ2: è®€å–å’Œæ­£è¦åŒ–\n",
    "    dataset = dataset.map(\n",
    "        read_and_normalize_image,\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "\n",
    "    # æ­¥é©Ÿ3: å¿«å–ï¼ˆåŠ é€Ÿè¨“ç·´ï¼‰\n",
    "    dataset = dataset.cache()\n",
    "    print(f\"  âœ“ å¿«å–åˆ°è¨˜æ†¶é«”\")\n",
    "\n",
    "    if is_training:\n",
    "        # æ­¥é©Ÿ4: æ‰“äº‚\n",
    "        dataset = dataset.shuffle(buffer_size=min(1000, len(image_paths)))\n",
    "        print(f\"  âœ“ æ‰“äº‚è³‡æ–™ï¼ˆbuffer={min(1000, len(image_paths))}ï¼‰\")\n",
    "\n",
    "        # æ­¥é©Ÿ5: è³‡æ–™å¢å¼·\n",
    "        dataset = dataset.map(\n",
    "            apply_augmentation,\n",
    "            num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "        print(f\"  âœ“ æ‡‰ç”¨è³‡æ–™å¢å¼·\")\n",
    "\n",
    "    # æ­¥é©Ÿ6: æ‰¹æ¬¡åŒ–\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=is_training)\n",
    "    print(f\"  âœ“ æ‰¹æ¬¡åŒ–ï¼ˆdrop_remainder={is_training}ï¼‰\")\n",
    "\n",
    "    # æ­¥é©Ÿ7: é å–\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    print(f\"  âœ“ é å–ä¸‹ä¸€æ‰¹è³‡æ–™\")\n",
    "\n",
    "    # æ­¥é©Ÿ8: é‡è¤‡ï¼ˆè¨“ç·´é›†ç„¡é™å¾ªç’°ï¼‰\n",
    "    if is_training:\n",
    "        dataset = dataset.repeat()\n",
    "        print(f\"  âœ“ ç„¡é™é‡è¤‡ï¼ˆç”¨æ–¼è¨“ç·´ï¼‰\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# å»ºç«‹ä¸‰å€‹è³‡æ–™é›†\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ—ï¸ å»ºç«‹è¨“ç·´/é©—è­‰/æ¸¬è©¦è³‡æ–™é›†...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_dataset = create_dataset(\n",
    "    train_images, train_masks,\n",
    "    is_training=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_dataset = create_dataset(\n",
    "    val_images, val_masks,\n",
    "    is_training=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_dataset = create_dataset(\n",
    "    test_images, test_masks,\n",
    "    is_training=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# è¨ˆç®—è¨“ç·´æ­¥æ•¸\n",
    "steps_per_epoch = len(train_images) // BATCH_SIZE\n",
    "validation_steps = len(val_images) // BATCH_SIZE\n",
    "test_steps = len(test_images) // BATCH_SIZE\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š è³‡æ–™é›†è³‡è¨Šæ‘˜è¦:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = {\n",
    "    'è³‡æ–™é›†': ['è¨“ç·´é›†', 'é©—è­‰é›†', 'æ¸¬è©¦é›†'],\n",
    "    'æ¨£æœ¬æ•¸é‡': [len(train_images), len(val_images), len(test_images)],\n",
    "    'æ‰¹æ¬¡æ•¸é‡': [steps_per_epoch, validation_steps, test_steps],\n",
    "    'æ‰¹æ¬¡å¤§å°': [BATCH_SIZE, BATCH_SIZE, BATCH_SIZE],\n",
    "    'è³‡æ–™å¢å¼·': ['âœ“', 'âœ—', 'âœ—'],\n",
    "    'æ‰“äº‚': ['âœ“', 'âœ—', 'âœ—'],\n",
    "    'é‡è¤‡': ['âœ“', 'âœ—', 'âœ—']\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "print(f\"\\nğŸ”¢ è¨“ç·´é…ç½®:\")\n",
    "print(f\"  æ¯è¼ªæ­¥æ•¸ (steps_per_epoch): {steps_per_epoch}\")\n",
    "print(f\"  é©—è­‰æ­¥æ•¸ (validation_steps): {validation_steps}\")\n",
    "print(f\"  æ¸¬è©¦æ­¥æ•¸ (test_steps): {test_steps}\")\n",
    "\n",
    "# æ¸¬è©¦è³‡æ–™ç®¡ç·š\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ§ª æ¸¬è©¦è³‡æ–™ç®¡ç·š...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å¾è¨“ç·´é›†å–ä¸€å€‹æ‰¹æ¬¡\n",
    "sample_batch_images, sample_batch_masks = next(iter(train_dataset))\n",
    "\n",
    "print(f\"\\næ‰¹æ¬¡è³‡è¨Š:\")\n",
    "print(f\"  å½±åƒæ‰¹æ¬¡å½¢ç‹€: {sample_batch_images.shape}\")\n",
    "print(f\"  é®ç½©æ‰¹æ¬¡å½¢ç‹€: {sample_batch_masks.shape}\")\n",
    "print(f\"  å½±åƒæ•¸æ“šé¡å‹: {sample_batch_images.dtype}\")\n",
    "print(f\"  é®ç½©æ•¸æ“šé¡å‹: {sample_batch_masks.dtype}\")\n",
    "print(f\"  å½±åƒå€¼ç¯„åœ: [{sample_batch_images.numpy().min():.3f}, {sample_batch_images.numpy().max():.3f}]\")\n",
    "print(f\"  é®ç½©å€¼ç¯„åœ: [{sample_batch_masks.numpy().min():.3f}, {sample_batch_masks.numpy().max():.3f}]\")\n",
    "\n",
    "# è¦–è¦ºåŒ–ä¸€å€‹æ‰¹æ¬¡\n",
    "print(\"\\nğŸ“Š è¦–è¦ºåŒ–è¨“ç·´æ‰¹æ¬¡ï¼ˆå‰4å€‹æ¨£æœ¬ï¼‰...\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 16))\n",
    "\n",
    "for i in range(min(4, BATCH_SIZE)):\n",
    "    # å–å‡ºå–®å€‹æ¨£æœ¬\n",
    "    img = sample_batch_images[i].numpy()\n",
    "    mask = sample_batch_masks[i].numpy()\n",
    "\n",
    "    # åæ­£è¦åŒ–å½±åƒ\n",
    "    img_display = img * dataset_std + dataset_mean\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "    # é¡¯ç¤ºå½±åƒ\n",
    "    axes[i, 0].imshow(img_display)\n",
    "    axes[i, 0].set_title(f'æ‰¹æ¬¡æ¨£æœ¬ {i+1} - å¢å¼·å¾Œå½±åƒ', fontsize=11, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # é¡¯ç¤ºé®ç½©\n",
    "    im = axes[i, 1].imshow(mask[:, :, 0], cmap='jet', vmin=0, vmax=1)\n",
    "    axes[i, 1].set_title(f'æ‰¹æ¬¡æ¨£æœ¬ {i+1} - å°æ‡‰é®ç½©', fontsize=11, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    if i == 0:\n",
    "        plt.colorbar(im, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ¸¬è©¦è¿­ä»£é€Ÿåº¦\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš¡ æ¸¬è©¦è³‡æ–™ç®¡ç·šé€Ÿåº¦...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import time\n",
    "\n",
    "# æ¸¬è©¦è®€å–10å€‹æ‰¹æ¬¡çš„æ™‚é–“\n",
    "print(f\"\\nè®€å– 10 å€‹æ‰¹æ¬¡...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i, (images, masks) in enumerate(train_dataset.take(10)):\n",
    "    pass\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ç¸½æ™‚é–“: {elapsed_time:.2f} ç§’\")\n",
    "print(f\"  å¹³å‡æ¯æ‰¹æ¬¡: {elapsed_time/10:.3f} ç§’\")\n",
    "print(f\"  ååé‡: {(BATCH_SIZE * 10) / elapsed_time:.1f} æ¨£æœ¬/ç§’\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æ•ˆèƒ½æç¤º:\")\n",
    "print(\"  - ä½¿ç”¨ cache() å¯é¿å…é‡è¤‡è®€å–ç£ç¢Ÿ\")\n",
    "print(\"  - ä½¿ç”¨ prefetch() å¯åœ¨ GPU è¨ˆç®—æ™‚é å…ˆæº–å‚™è³‡æ–™\")\n",
    "print(\"  - ä½¿ç”¨ num_parallel_calls å¯å¹³è¡Œè™•ç†è³‡æ–™\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Dataset ç®¡ç·šå»ºç«‹å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T02:59:40.067092Z",
     "iopub.status.idle": "2025-10-18T02:59:40.067303Z",
     "shell.execute_reply": "2025-10-18T02:59:40.067208Z",
     "shell.execute_reply.started": "2025-10-18T02:59:40.067197Z"
    },
    "id": "PazML9Ddzzf-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Cell 13: ç”Ÿæˆå‰è™•ç†å ±å‘Šèˆ‡å„²å­˜è¨­å®š ====================\n",
    "print(\"ğŸ“‹ ç”Ÿæˆè³‡æ–™å‰è™•ç†å®Œæ•´å ±å‘Š...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æ”¶é›†æ‰€æœ‰å‰è™•ç†è³‡è¨Š\n",
    "preprocessing_report = {\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(final_images),\n",
    "        'train_samples': len(train_images),\n",
    "        'val_samples': len(val_images),\n",
    "        'test_samples': len(test_images),\n",
    "        'image_size': IMAGE_SIZE,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'unique_labels': unique_labels,\n",
    "    },\n",
    "    'normalization': {\n",
    "        'method': 'dataset_statistics',\n",
    "        'mean_rgb': dataset_mean.tolist(),\n",
    "        'std_rgb': dataset_std.tolist(),\n",
    "    },\n",
    "    'augmentation': AUGMENTATION_CONFIG,\n",
    "    'class_weights': class_weight_dict,\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'steps_per_epoch': steps_per_epoch,\n",
    "        'validation_steps': validation_steps,\n",
    "        'test_steps': test_steps,\n",
    "    },\n",
    "    'class_distribution': {\n",
    "        'pixel_counts': dict(pixel_counts),\n",
    "        'occurrences': dict(occurrences),\n",
    "        'imbalance_ratio': float(imbalance_ratio),\n",
    "    }\n",
    "}\n",
    "\n",
    "# é¡¯ç¤ºå®Œæ•´å ±å‘Š\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š è³‡æ–™å‰è™•ç†å®Œæ•´å ±å‘Š\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nã€1. è³‡æ–™é›†è³‡è¨Šã€‘\")\n",
    "print(f\"  ç¸½æ¨£æœ¬æ•¸: {preprocessing_report['dataset_info']['total_samples']}\")\n",
    "print(f\"  è¨“ç·´é›†: {preprocessing_report['dataset_info']['train_samples']} ({len(train_images)/len(final_images)*100:.1f}%)\")\n",
    "print(f\"  é©—è­‰é›†: {preprocessing_report['dataset_info']['val_samples']} ({len(val_images)/len(final_images)*100:.1f}%)\")\n",
    "print(f\"  æ¸¬è©¦é›†: {preprocessing_report['dataset_info']['test_samples']} ({len(test_images)/len(final_images)*100:.1f}%)\")\n",
    "print(f\"  å½±åƒå°ºå¯¸: {IMAGE_SIZE}Ã—{IMAGE_SIZE}\")\n",
    "print(f\"  é¡åˆ¥æ•¸é‡: {NUM_CLASSES}\")\n",
    "\n",
    "print(\"\\nã€2. æ­£è¦åŒ–åƒæ•¸ã€‘\")\n",
    "print(f\"  æ–¹æ³•: è³‡æ–™é›†çµ±è¨ˆé‡æ­£è¦åŒ–\")\n",
    "print(f\"  RGB å‡å€¼: {dataset_mean}\")\n",
    "print(f\"  RGB æ¨™æº–å·®: {dataset_std}\")\n",
    "\n",
    "print(\"\\nã€3. è³‡æ–™å¢å¼·é…ç½®ã€‘\")\n",
    "for key, value in AUGMENTATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nã€4. é¡åˆ¥æ¬Šé‡ï¼ˆå‰5å€‹ï¼‰ã€‘\")\n",
    "for i, (class_id, weight) in enumerate(list(class_weight_dict.items())[:5]):\n",
    "    print(f\"  é¡åˆ¥ {class_id}: {weight:.4f}\")\n",
    "print(f\"  ... å…± {len(class_weight_dict)} å€‹é¡åˆ¥\")\n",
    "\n",
    "print(\"\\nã€5. è¨“ç·´é…ç½®ã€‘\")\n",
    "print(f\"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}\")\n",
    "print(f\"  æ¯è¼ªæ­¥æ•¸: {steps_per_epoch}\")\n",
    "print(f\"  é©—è­‰æ­¥æ•¸: {validation_steps}\")\n",
    "print(f\"  æ¸¬è©¦æ­¥æ•¸: {test_steps}\")\n",
    "\n",
    "print(\"\\nã€6. é¡åˆ¥ä¸å¹³è¡¡ã€‘\")\n",
    "print(f\"  ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio > 100:\n",
    "    print(f\"  âš ï¸ åš´é‡ä¸å¹³è¡¡ï¼Œå·²è¨ˆç®—é¡åˆ¥æ¬Šé‡\")\n",
    "elif imbalance_ratio > 10:\n",
    "    print(f\"  âš ï¸ ä¸­åº¦ä¸å¹³è¡¡ï¼Œå»ºè­°ä½¿ç”¨é¡åˆ¥æ¬Šé‡\")\n",
    "else:\n",
    "    print(f\"  âœ“ ç›¸å°å¹³è¡¡\")\n",
    "\n",
    "# å„²å­˜ç‚º JSON\n",
    "import json\n",
    "\n",
    "report_filename = 'preprocessing_report.json'\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(preprocessing_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nğŸ’¾ å ±å‘Šå·²å„²å­˜è‡³: {report_filename}\")\n",
    "\n",
    "# å»ºç«‹è¦–è¦ºåŒ–ç¸½çµ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ˆ è¦–è¦ºåŒ–ç¸½çµå ±å‘Š...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. è³‡æ–™é›†åˆ†å‰²\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sets = ['è¨“ç·´é›†', 'é©—è­‰é›†', 'æ¸¬è©¦é›†']\n",
    "sizes = [len(train_images), len(val_images), len(test_images)]\n",
    "colors_pie = ['#4CAF50', '#2196F3', '#FF9800']\n",
    "ax1.pie(sizes, labels=sets, colors=colors_pie, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('è³‡æ–™é›†åˆ†å‰²', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. é¡åˆ¥æ•¸é‡\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.text(0.5, 0.7, f'{NUM_CLASSES}', ha='center', va='center',\n",
    "         fontsize=60, fontweight='bold', color='#2196F3')\n",
    "ax2.text(0.5, 0.3, 'é¡åˆ¥ç¸½æ•¸', ha='center', va='center',\n",
    "         fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "\n",
    "# 3. ä¸å¹³è¡¡æ¯”ä¾‹\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "color = '#F44336' if imbalance_ratio > 100 else '#FF9800' if imbalance_ratio > 10 else '#4CAF50'\n",
    "ax3.text(0.5, 0.7, f'{imbalance_ratio:.1f}:1', ha='center', va='center',\n",
    "         fontsize=40, fontweight='bold', color=color)\n",
    "ax3.text(0.5, 0.3, 'é¡åˆ¥ä¸å¹³è¡¡æ¯”', ha='center', va='center',\n",
    "         fontsize=14, fontweight='bold')\n",
    "ax3.set_xlim(0, 1)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.axis('off')\n",
    "\n",
    "# 4. é¡åˆ¥åƒç´ åˆ†ä½ˆ\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "labels_bar = [str(l) for l in unique_labels]\n",
    "pixels_bar = [pixel_counts[l] for l in unique_labels]\n",
    "bars = ax4.bar(labels_bar, pixels_bar, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax4.set_xlabel('é¡åˆ¥ ID', fontsize=12)\n",
    "ax4.set_ylabel('ç¸½åƒç´ æ•¸ï¼ˆå°æ•¸åˆ»åº¦ï¼‰', fontsize=12)\n",
    "ax4.set_title('å„é¡åˆ¥åƒç´ åˆ†ä½ˆ', fontsize=14, fontweight='bold')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# 5. è¨“ç·´é…ç½®\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "config_text = f\"\"\"\n",
    "è¨“ç·´é…ç½®\n",
    "\n",
    "æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}\n",
    "æ¯è¼ªæ­¥æ•¸: {steps_per_epoch}\n",
    "é©—è­‰æ­¥æ•¸: {validation_steps}\n",
    "å½±åƒå°ºå¯¸: {IMAGE_SIZE}Ã—{IMAGE_SIZE}\n",
    "\"\"\"\n",
    "ax5.text(0.5, 0.5, config_text, ha='center', va='center',\n",
    "         fontsize=11, family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "ax5.set_xlim(0, 1)\n",
    "ax5.set_ylim(0, 1)\n",
    "ax5.axis('off')\n",
    "\n",
    "# 6. æ­£è¦åŒ–åƒæ•¸\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "norm_text = f\"\"\"\n",
    "æ­£è¦åŒ–åƒæ•¸\n",
    "\n",
    "æ–¹æ³•: Dataset Statistics\n",
    "\n",
    "RGB å‡å€¼:\n",
    "R: {dataset_mean[0]:.4f}\n",
    "G: {dataset_mean[1]:.4f}\n",
    "B: {dataset_mean[2]:.4f}\n",
    "\n",
    "RGB æ¨™æº–å·®:\n",
    "R: {dataset_std[0]:.4f}\n",
    "G: {dataset_std[1]:.4f}\n",
    "B: {dataset_std[2]:.4f}\n",
    "\"\"\"\n",
    "ax6.text(0.5, 0.5, norm_text, ha='center', va='center',\n",
    "         fontsize=10, family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "ax6.set_xlim(0, 1)\n",
    "ax6.set_ylim(0, 1)\n",
    "ax6.axis('off')\n",
    "\n",
    "# 7. å¢å¼·é…ç½®\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "aug_text = \"è³‡æ–™å¢å¼·é…ç½®\\n\\n\"\n",
    "for key, value in AUGMENTATION_CONFIG.items():\n",
    "    short_key = key.replace('_prob', '').replace('_', ' ').title()\n",
    "    aug_text += f\"{short_key}: {value}\\n\"\n",
    "ax7.text(0.5, 0.5, aug_text, ha='center', va='center',\n",
    "         fontsize=10, family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "ax7.set_xlim(0, 1)\n",
    "ax7.set_ylim(0, 1)\n",
    "ax7.axis('off')\n",
    "\n",
    "plt.suptitle('DeepFashion è³‡æ–™å‰è™•ç†å®Œæ•´å ±å‘Š', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.savefig('preprocessing_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¾ è¦–è¦ºåŒ–å ±å‘Šå·²å„²å­˜è‡³: preprocessing_summary.png\")\n",
    "\n",
    "# é¡¯ç¤ºä½¿ç”¨èªªæ˜\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ æ¨¡å‹è¨“ç·´æ™‚çš„ä½¿ç”¨æ–¹å¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "usage_code = \"\"\"\n",
    "# åœ¨æ¨¡å‹è¨“ç·´æ™‚ä½¿ç”¨ä»¥ä¸‹é…ç½®:\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES)]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weight_dict,  # ä½¿ç”¨é¡åˆ¥æ¬Šé‡\n",
    "    callbacks=[...]\n",
    ")\n",
    "\n",
    "# è©•ä¼°æ¨¡å‹\n",
    "test_results = model.evaluate(test_dataset, steps=test_steps)\n",
    "\"\"\"\n",
    "\n",
    "print(usage_code)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… æ‰€æœ‰è³‡æ–™å‰è™•ç†å®Œæˆï¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“¦ å¯ç”¨çš„è®Šæ•¸:\")\n",
    "print(\"  - train_dataset: è¨“ç·´è³‡æ–™é›†\")\n",
    "print(\"  - val_dataset: é©—è­‰è³‡æ–™é›†\")\n",
    "print(\"  - test_dataset: æ¸¬è©¦è³‡æ–™é›†\")\n",
    "print(\"  - NUM_CLASSES: é¡åˆ¥æ•¸é‡\")\n",
    "print(\"  - class_weight_dict: é¡åˆ¥æ¬Šé‡å­—å…¸\")\n",
    "print(\"  - steps_per_epoch: æ¯è¼ªè¨“ç·´æ­¥æ•¸\")\n",
    "print(\"  - validation_steps: é©—è­‰æ­¥æ•¸\")\n",
    "print(\"  - dataset_mean: è³‡æ–™é›†å‡å€¼\")\n",
    "print(\"  - dataset_std: è³‡æ–™é›†æ¨™æº–å·®\")\n",
    "\n",
    "print(\"\\nğŸ¯ ä¸‹ä¸€æ­¥: å»ºç«‹ä¸¦è¨“ç·´æ¨¡å‹ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
